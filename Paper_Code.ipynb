{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "E1Z_-NwMfgiJ",
        "zjnnR1EVsBLD",
        "44e4dcc1"
      ],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOMYNx0MuIbNWfDJX5I3JGs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/00000281892/DynaBiome/blob/main/Paper_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "951a266f"
      },
      "source": [
        "# Project Title\n",
        "\n",
        "Anomaly Detection in Longitudinal Microbiome Data using LSTM Autoencoders and Ensemble Learning.\n",
        "\n",
        "## Overview\n",
        "\n",
        "This reserach project implements an anomaly detection framework using LSTM Autoencoders to identify anomalous patterns in longitudinal microbiome data. The reconstruction error from the autoencoder is then used as a feature for various downstream classification models (Logistic Regression, MLP, One-Class SVM, K-Nearest Neighbors, XGBoost, Random Forest) and ensemble learning techniques (Averaged Probabilities, Weighted Averaging, Stacking) to improve anomaly detection performance.\n",
        "\n",
        "The project includes:\n",
        "- Data loading and preprocessing for time-series analysis.\n",
        "- Training of an LSTM Autoencoder on normal data sequences.\n",
        "- Evaluation of the autoencoder using reconstruction error analysis and optimal thresholding.\n",
        "- Training and evaluation of individual classifiers on reconstruction errors.\n",
        "- Implementation and evaluation of ensemble learning methods.\n",
        "- Statistical testing and bootstrap analysis to compare model performances.\n",
        "- Visualization of results including reconstruction error distribution, ROC and PR curves, confusion matrices, and SHAP explanations.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Setup](#setup)\n",
        "2. [Data](#data)\n",
        "3. [Methodology](#methodology)\n",
        "4. [Results](#results)\n",
        "5. [Interpretation](#interpretation)\n",
        "6. [Ensemble Learning](#ensemble-learning)\n",
        "7. [Statistical Analysis](#statistical-analysis)\n",
        "8. [File Structure](#file-structure)\n",
        "9. [Dependencies](#dependencies)\n",
        "10. [Usage](#usage)\n",
        "11. [License](#license)\n",
        "12. [Contact](#contact)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"setup\"></a>\n",
        "## setup"
      ],
      "metadata": {
        "id": "E1Z_-NwMfgiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install shap # Install the shap library before importing it.\n",
        "!pip install statsmodels"
      ],
      "metadata": {
        "id": "H1Wmype3JBPY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "o8PsXWfdC1Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"data\"></a>\n",
        "## Data"
      ],
      "metadata": {
        "id": "zjnnR1EVsBLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load csv file with pandas\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/asv_interpretability_dataset_modified.csv', dtype={'PatientID': str})"
      ],
      "metadata": {
        "id": "_-5UkEIYST_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"\"></a>"
      ],
      "metadata": {
        "id": "kKcc4c6wsL5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"methodology\"></a>\n",
        "## Methodology"
      ],
      "metadata": {
        "id": "mL0aBcs2smCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Handle NeutrophilCount with '<0.1' values ---\n",
        "def parse_neutrophil(value):\n",
        "    try:\n",
        "        return float(value)\n",
        "    except:\n",
        "        if isinstance(value, str) and \"<\" in value:\n",
        "            threshold = float(value.replace(\"<\", \"\").strip())\n",
        "            return threshold / 2  # or use threshold itself\n",
        "        return np.nan\n",
        "\n",
        "df['NeutrophilCount'] = df['NeutrophilCount'].apply(parse_neutrophil)"
      ],
      "metadata": {
        "id": "8BSLb9Phcszr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode stool consistency\n",
        "df = pd.get_dummies(df, columns=['Consistency'])"
      ],
      "metadata": {
        "id": "6iVEIH8pmlH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log transform Genus-relative abundances\n",
        "df['RelativeAbundance'] = df['RelativeAbundance'].astype(float)\n",
        "df['RelativeAbundance'] = np.log1p(df['RelativeAbundance'])"
      ],
      "metadata": {
        "id": "GQXT83Lfmof1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Newly added section for testing - ground truth label\n",
        "def label_dysbiosis(row):\n",
        "    is_temp_abnormal = row['MaxTemperature'] > 38.0\n",
        "    is_neutro_low = row['NeutrophilCount'] < 500\n",
        "    is_consistency_liquid = row.get('Consistency_liquid', 0) == 1\n",
        "    return int(is_temp_abnormal and is_neutro_low and is_consistency_liquid)"
      ],
      "metadata": {
        "id": "oeWD-lY5bMi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['DysbiosisLabel'] = df.apply(label_dysbiosis, axis=1)"
      ],
      "metadata": {
        "id": "JbyaZBdnbNsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep metadata along with abundances\n",
        "metadata_cols = ['PatientID', 'SampleID', 'DayRelativeToNearestHCT',\n",
        "                 'MaxTemperature', 'NeutrophilCount'] + \\\n",
        "                [col for col in df.columns if col.startswith('Consistency_')] + \\\n",
        "                ['DysbiosisLabel']"
      ],
      "metadata": {
        "id": "4Hs5b-NObmR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pivot genus into columns (wide format)\n",
        "genus_pivot = df.pivot_table(index=['PatientID', 'SampleID', 'DayRelativeToNearestHCT'],\n",
        "                              columns='Genus', values='RelativeAbundance', fill_value=0).reset_index()"
      ],
      "metadata": {
        "id": "vjiIFOJLmrEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge with metadata\n",
        "metadata = df[metadata_cols].drop_duplicates(subset=['PatientID', 'SampleID', 'DayRelativeToNearestHCT'])\n",
        "merged_df = pd.merge(genus_pivot, metadata, on=['PatientID', 'SampleID', 'DayRelativeToNearestHCT'], how='left')"
      ],
      "metadata": {
        "id": "annLq-GbdkmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop genus columns with zero variance\n",
        "genus_cols = df['Genus'].unique().tolist()\n",
        "variances = merged_df[genus_cols].var()\n",
        "non_zero_var_cols = variances[variances > 1e-6].index.tolist()"
      ],
      "metadata": {
        "id": "qD36SFD4m6DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final feature set\n",
        "feature_cols = non_zero_var_cols + ['MaxTemperature', 'NeutrophilCount'] + \\\n",
        "               [col for col in merged_df.columns if 'Consistency' in col]"
      ],
      "metadata": {
        "id": "3JxkhOB2m8ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== STEP 2: Scaling ====================\n",
        "scaler = MinMaxScaler()\n",
        "merged_df[feature_cols] = scaler.fit_transform(merged_df[feature_cols])"
      ],
      "metadata": {
        "id": "XziaunbinBWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== STEP 3: Prepare Time Series ====================\n",
        "# Sort & reshape by patient and day\n",
        "merged_df = merged_df.sort_values(['PatientID', 'DayRelativeToNearestHCT'])"
      ],
      "metadata": {
        "id": "Mk17zTIpnI5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- [1] Build sequences with labels ---\n",
        "def build_sequences_with_labels(df, feature_cols, label_col='DysbiosisLabel', seq_len=14):\n",
        "    X_sequences = []\n",
        "    y_labels = []\n",
        "    time_indices = []\n",
        "\n",
        "    for pid, group in df.groupby('PatientID'):\n",
        "        group = group.sort_values('DayRelativeToNearestHCT')\n",
        "        values = group[feature_cols].values\n",
        "        labels = group[label_col].values\n",
        "        days = group['DayRelativeToNearestHCT'].values\n",
        "\n",
        "        for i in range(len(values) - seq_len + 1):\n",
        "            seq = values[i:i+seq_len]\n",
        "            label_window = labels[i:i+seq_len]\n",
        "            label = int(label_window.max())  # 1 if any point in window is dysbiotic\n",
        "            time = days[i+seq_len-1]  # assign the last day of the window\n",
        "            X_sequences.append(seq)\n",
        "            y_labels.append(label)\n",
        "            time_indices.append((pid, time))\n",
        "\n",
        "    return np.array(X_sequences), np.array(y_labels), time_indices"
      ],
      "metadata": {
        "id": "nS47kMJTf2px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- [2] Define features ---\n",
        "seq_len = 14\n",
        "feature_cols = [col for col in merged_df.columns\n",
        "                if col not in ['PatientID', 'SampleID', 'DayRelativeToNearestHCT',\n",
        "                               'DysbiosisLabel', 'MaxTemperature', 'NeutrophilCount']\n",
        "                and not col.startswith('Consistency_')]\n",
        "\n",
        "# --- [3] Build sequences ---\n",
        "X_seq, y_seq, time_idx = build_sequences_with_labels(merged_df, feature_cols, seq_len=seq_len)"
      ],
      "metadata": {
        "id": "xEy9deVif62a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- [4] Create DataFrame for temporal splitting ---\n",
        "split_df = pd.DataFrame(time_idx, columns=['PatientID', 'DayRelativeToNearestHCT'])\n",
        "split_df['y'] = y_seq\n",
        "split_df['X_idx'] = range(len(X_seq))"
      ],
      "metadata": {
        "id": "hn6XpL4NgAOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- [5] Sort by patient and time ---\n",
        "split_df = split_df.sort_values(['PatientID', 'DayRelativeToNearestHCT'])"
      ],
      "metadata": {
        "id": "qwH2IDMCgBE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- [6] Determine split indices (70/15/15 by number of sequences) ---\n",
        "n_total = len(split_df)\n",
        "n_train = int(0.7 * n_total)\n",
        "n_val = int(0.15 * n_total)"
      ],
      "metadata": {
        "id": "5Q3JbPsagE5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_idx = split_df.iloc[:n_train]['X_idx'].values\n",
        "val_idx = split_df.iloc[n_train:n_train+n_val]['X_idx'].values\n",
        "test_idx = split_df.iloc[n_train+n_val:]['X_idx'].values"
      ],
      "metadata": {
        "id": "WKf5yEtsgFfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- [7] Final splits ---\n",
        "X_train, y_train = X_seq[train_idx], y_seq[train_idx]\n",
        "X_val, y_val = X_seq[val_idx], y_seq[val_idx]\n",
        "X_test, y_test = X_seq[test_idx], y_seq[test_idx]"
      ],
      "metadata": {
        "id": "f8F_6dyMgI0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "YtC00ERBgKeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter X_train to only include normal samples (where y_train is 0)\n",
        "X_train_normal = X_train[y_train == 0]"
      ],
      "metadata": {
        "id": "PVIX2-lTLjjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = X_train_normal.shape[1]\n",
        "n_features = X_train_normal.shape[2]\n",
        "\n",
        "print(f\"Timesteps: {timesteps}, Features: {n_features}\")"
      ],
      "metadata": {
        "id": "zS65iK58jCm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== STEP 4: LSTM Autoencoder ====================\n",
        "inputs = Input(shape=(timesteps, n_features))\n",
        "encoded = LSTM(128, return_sequences=True)(inputs)\n",
        "encoded = LSTM(64, return_sequences=False)(encoded)\n",
        "bottleneck = RepeatVector(timesteps)(encoded)\n",
        "decoded = LSTM(64, return_sequences=True)(bottleneck)\n",
        "decoded = LSTM(128, return_sequences=True)(decoded)\n",
        "outputs = TimeDistributed(Dense(n_features))(decoded)\n",
        "\n",
        "autoencoder = Model(inputs, outputs)\n",
        "autoencoder.compile(optimizer=Adam(1e-3), loss='mae')"
      ],
      "metadata": {
        "id": "PtOZfg1rldFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "DQZE0vqeldF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Save best model based on lowest val_loss\n",
        "checkpoint_cb = ModelCheckpoint(\n",
        "    filepath=\"best_model_Unsupervised.keras\",\n",
        "    monitor=\"val_loss\",\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "IU5qXe3GldF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = autoencoder.fit(X_train_normal , X_train_normal , epochs=300, batch_size=32, validation_data=(X_val, X_val), callbacks=[early_stop, checkpoint_cb], verbose=1)"
      ],
      "metadata": {
        "id": "0s5LmpR4mfwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "from tensorflow.keras.models import load_model\n",
        "best_autoencoder = load_model('best_model_Unsupervised.keras')"
      ],
      "metadata": {
        "id": "qx4fU-0a6IYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"results\"></a>\n",
        "## Results"
      ],
      "metadata": {
        "id": "qpLh5usNtRX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconstruction loss trends during training and validation"
      ],
      "metadata": {
        "id": "MhqVHVSkPUFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Reconstruction Loss Trends', fontsize=16)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss (MAE)', fontsize=12)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "plt.savefig('reconstruction_loss_trends.pdf', dpi=600, format='pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m4FepV4Dnp_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconstruction Errors on Validation Set"
      ],
      "metadata": {
        "id": "Z_DYzKqrGPgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'autoencoder_model' is your trained autoencoder\n",
        "# Assuming 'X_val' is your validation data\n",
        "\n",
        "# Predict the reconstruction of X_val using the trained autoencoder\n",
        "X_val_pred = autoencoder.predict(X_val)\n",
        "\n",
        "# Calculate the Mean Absolute Error (MAE) between the original X_val and its reconstruction\n",
        "reconstruction_errors_val = np.mean(np.abs(X_val_pred - X_val), axis=(1, 2))\n",
        "\n",
        "# reconstruction_errors_val now contains a single reconstruction error value for each sequence in X_val\n",
        "\n",
        "threshold = 0.0003 # np.percentile(reconstruction_errors_val, 95)\n",
        "# Newly Added on 11 May, 2025\n",
        "# predicted_labels = (reconstruction_errors >= threshold).astype(int)\n",
        "\n",
        "print(f\"\\nAnomaly detection threshold (95th percentile): {threshold:.6f}\")"
      ],
      "metadata": {
        "id": "SHCrfMDSd-o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconstruction Errors on Test Set"
      ],
      "metadata": {
        "id": "oGxJ1UoeGV4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_pred = autoencoder.predict(X_test)\n",
        "reconstruction_errors_test = np.mean(np.abs(X_test_pred - X_test), axis=(1, 2))\n",
        "anomaly_predictions = reconstruction_errors_test > threshold  # threshold from training on validation set. Use the Threshold obtained via validation set\n",
        "print(f\"Number of flagged sequences: {anomaly_predictions.sum()} out of {len(anomaly_predictions)}\")"
      ],
      "metadata": {
        "id": "2iOv2LVEjbnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. Find Optimal Threshold on the Validation Set ---\n",
        "\n",
        "# Compute ROC Curve on Validation Set to get thresholds\n",
        "fpr_val, tpr_val, thresholds_val = roc_curve(y_val, reconstruction_errors_val)\n",
        "\n",
        "# Find the best threshold using Youden's index on Validation Set\n",
        "optimal_idx_val = np.argmax(tpr_val - fpr_val)\n",
        "optimal_threshold = thresholds_val[optimal_idx_val]\n",
        "\n",
        "print(f\"Optimal Threshold for anomaly detection (from Validation Set): {optimal_threshold:.4f}\")\n",
        "\n",
        "# --- 2. Evaluate Performance and Plot on the Test Set using the Optimal Threshold ---\n",
        "\n",
        "# Classify Test Instances using the optimal threshold found on the validation set\n",
        "anomaly_predictions_test = (reconstruction_errors_test > optimal_threshold).astype(int)\n",
        "\n",
        "# Compute Evaluation Metrics on the Test Set\n",
        "roc_auc_test = roc_auc_score(y_test, reconstruction_errors_test) # AUC is calculated from scores, not binary predictions\n",
        "pr_auc_test = average_precision_score(y_test, reconstruction_errors_test) # PR AUC is calculated from scores\n",
        "\n",
        "# You can also calculate metrics based on the binary predictions\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "precision_test = precision_score(y_test, anomaly_predictions_test, zero_division=1)\n",
        "recall_test = recall_score(y_test, anomaly_predictions_test, zero_division=1)\n",
        "f1_test = f1_score(y_test, anomaly_predictions_test, zero_division=1)\n",
        "\n",
        "print(\"\\nEvaluation on Test Set using Optimal Threshold from Validation:\")\n",
        "print(f\"Precision: {precision_test:.4f}\")\n",
        "print(f\"Recall: {recall_test:.4f}\")\n",
        "print(f\"F1-Score: {f1_test:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_test:.4f}\")\n",
        "print(f\"PR AUC: {pr_auc_test:.4f}\")\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(y_test, anomaly_predictions_test, zero_division=1))\n",
        "\n",
        "\n",
        "# --- 3. Plot ROC Curve on the Test Set ---\n",
        "\n",
        "# Compute ROC Curve on Test Set (for plotting the curve itself)\n",
        "fpr_test, tpr_test, thresholds_test_plot = roc_curve(y_test, reconstruction_errors_test)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# --- ROC ---\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(fpr_test, tpr_test, label=f'ROC AUC = {roc_auc_test:.3f}', color='blue')\n",
        "\n",
        "# Find the point on the Test ROC curve corresponding to the optimal threshold from Validation\n",
        "# This requires finding the index in thresholds_test_plot closest to optimal_threshold\n",
        "closest_threshold_idx_test = np.argmin(np.abs(thresholds_test_plot - optimal_threshold))\n",
        "plt.scatter(fpr_test[closest_threshold_idx_test], tpr_test[closest_threshold_idx_test],\n",
        "            color='red', label=f'Applied Threshold ({optimal_threshold:.4f})', zorder=5) # zorder to ensure point is visible\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal reference line\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve (Evaluated on Test Set)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Save the ROC plot\n",
        "# Define a suitable output directory if not already defined\n",
        "output_dir_evaluation_plots = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Evaluation_Plots/\"\n",
        "import os\n",
        "os.makedirs(output_dir_evaluation_plots, exist_ok=True)\n",
        "\n",
        "plt.savefig(f\"{output_dir_evaluation_plots}/ROC_Curve_TestSet_ValidatedThreshold.pdf\", dpi=600, bbox_inches='tight')\n",
        "\n",
        "# --- 4. Plot Precision-Recall Curve on the Test Set ---\n",
        "\n",
        "# Compute PR Curve on Test Set (for plotting the curve itself)\n",
        "precision_test_plot, recall_test_plot, thresholds_pr_test_plot = precision_recall_curve(y_test, reconstruction_errors_test)\n",
        "\n",
        "plt.subplot(1, 2, 2) # Plotting PR curve in the second subplot\n",
        "plt.plot(recall_test_plot, precision_test_plot, label=f'PR AUC = {pr_auc_test:.3f}', color='red')\n",
        "\n",
        "# Find the point on the Test PR curve corresponding to the optimal threshold from Validation\n",
        "# Similar to ROC, find the index in thresholds_pr_test_plot closest to optimal_threshold\n",
        "# Note: precision_recall_curve thresholds are slightly different in number/values than roc_curve\n",
        "closest_threshold_idx_pr_test = np.argmin(np.abs(thresholds_pr_test_plot - optimal_threshold))\n",
        "# Need to handle the case where thresholds_pr_test_plot might be empty (e.g., all samples are the same)\n",
        "if thresholds_pr_test_plot.size > 0:\n",
        "     plt.scatter(recall_test_plot[closest_threshold_idx_pr_test], precision_test_plot[closest_threshold_idx_pr_test],\n",
        "                 color='blue', label=f'Applied Threshold ({optimal_threshold:.4f})', zorder=5)\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve (Evaluated on Test Set)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Save the PR plot\n",
        "plt.savefig(f\"{output_dir_evaluation_plots}/PR_Curve_TestSet_ValidatedThreshold.pdf\", dpi=600, bbox_inches='tight')\n",
        "\n",
        "\n",
        "plt.tight_layout() # Adjust layout to prevent overlap\n",
        "plt.show()\n",
        "\n",
        "# --- 5. Plot Confusion Matrix on the Test Set ---\n",
        "cm_test = confusion_matrix(y_test, anomaly_predictions_test)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_test, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Predicted Normal', 'Predicted Anomaly'],\n",
        "            yticklabels=['Actual Normal', 'Actual Anomaly'])\n",
        "plt.title('Confusion Matrix (Evaluated on Test Set)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.savefig(f\"{output_dir_evaluation_plots}/ConfusionMatrix_TestSet_ValidatedThreshold.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "m1-yudzG8tNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconstruction Error Distribution using optimal threshold."
      ],
      "metadata": {
        "id": "u2d0GjHARb-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Reconstruction Error Distribution\n",
        "plt.figure(figsize=(10,4))\n",
        "# Filter reconstruction errors for normal (0) and anomaly (1) based on actual y_test labels\n",
        "reconstruction_errors_normal = reconstruction_errors_test[y_test == 0]\n",
        "reconstruction_errors_anomaly = reconstruction_errors_test[y_test == 1]\n",
        "\n",
        "# Plot histograms for both classes\n",
        "plt.hist(reconstruction_errors_normal, bins=50, color='skyblue', edgecolor='black', alpha=0.7, label='Actual Normal')\n",
        "plt.hist(reconstruction_errors_anomaly, bins=50, color='salmon', edgecolor='black', alpha=0.7, label='Actual Anomaly')\n",
        "\n",
        "# Add the optimal threshold line from validation set\n",
        "plt.axvline(optimal_threshold, color='red', linestyle='--', label=f'Optimal Threshold ({optimal_threshold:.4f})')\n",
        "\n",
        "plt.title(\"Reconstruction Error Distribution by Actual Class (Evaluated on Test Set)\")\n",
        "plt.xlabel(\"Reconstruction Error (MAE)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the distribution plot\n",
        "plt.savefig(f\"{output_dir_evaluation_plots}/reconstruction_error_distribution_by_class_TestSet_ValidatedThreshold.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Jv35VV3xRaO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"\"></a>\n",
        "## Interpretation"
      ],
      "metadata": {
        "id": "gjEQUoJIt7dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Define a wrapper model to calculate reconstruction error (if not already defined and accessible)\n",
        "# If ReconstructionErrorModel is defined in a previous cell and accessible, you can skip this part.\n",
        "class ReconstructionErrorModel:\n",
        "    def __init__(self, autoencoder): # Assuming autoencoder is available in the environment\n",
        "        self.autoencoder = autoencoder\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Reshape X to 3D before passing to autoencoder\n",
        "        # Assuming timesteps and n_features are defined globally and accessible\n",
        "        X_reshaped = X.reshape(-1, timesteps, n_features)\n",
        "\n",
        "        # Calculate reconstruction errors\n",
        "        X_pred = self.autoencoder.predict(X_reshaped)\n",
        "        reconstruction_error = np.mean(np.abs(X_reshaped - X_pred), axis=(1, 2))  # Scalar per sample\n",
        "        return reconstruction_error\n",
        "\n",
        "# Initialize the wrapper model (assuming autoencoder is available in the environment)\n",
        "# If 'autoencoder' and 'n_features' are not globally accessible, you need to ensure they are loaded/defined.\n",
        "try:\n",
        "    # This line assumes 'autoencoder' was loaded successfully in a previous cell\n",
        "    reconstruction_model = ReconstructionErrorModel(autoencoder)\n",
        "except NameError:\n",
        "    print(\"Error: 'autoencoder' model not found. Please ensure it is loaded before this cell.\")\n",
        "    # You might need to load the model here if it's not guaranteed to be in the environment\n",
        "    # from tensorflow.keras.models import load_model\n",
        "    # autoencoder = load_model(\"Proposed_Model_Binary_FullUnsupervised.keras\")\n",
        "    # reconstruction_model = ReconstructionErrorModel(autoencoder)\n",
        "    # You might also need to define n_features if it's not globally accessible\n",
        "    # n_features = X_train_normal.shape[2] # Or get from model input shape\n",
        "\n",
        "# Initialize SHAP explainer (Using a subset of training data as background data)\n",
        "# Assuming X_train is available from previous steps\n",
        "try:\n",
        "    rng = np.random.default_rng(seed=42)\n",
        "    # Use a subset for efficiency and reshape background data\n",
        "    explainer = shap.KernelExplainer(reconstruction_model.predict, X_train[:100].reshape(X_train[:100].shape[0], -1), rng=rng)\n",
        "except NameError:\n",
        "    print(\"Error: 'X_train' or 'reconstruction_model' not found. Ensure previous steps ran successfully.\")\n",
        "    # Handle the error appropriately, perhaps by exiting or loading necessary data"
      ],
      "metadata": {
        "id": "1WSGc51i_d5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# ----- Preprocess: Reshape and Get SHAP Values -----\n",
        "# Only explain the first 10 test samples\n",
        "X_test_reshape = X_test[:10].reshape(X_test[:10].shape[0], -1)\n",
        "shap_values_test = explainer.shap_values(X_test_reshape)\n",
        "\n",
        "# ----- Generate Feature Names Across Time Steps -----\n",
        "flattened_feature_names = [\n",
        "    f\"{feature}_t{t}\" for t in range(timesteps) for feature in feature_cols\n",
        "]"
      ],
      "metadata": {
        "id": "aXDqEGbM9fbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Set High-Quality Plotting Parameters -----\n",
        "sns.set(style=\"whitegrid\", font_scale=1.5)\n",
        "plt.rcParams.update({\n",
        "    \"axes.titlesize\": 20,\n",
        "    \"axes.labelsize\": 16,\n",
        "    \"xtick.labelsize\": 12,\n",
        "    \"ytick.labelsize\": 12,\n",
        "    \"legend.fontsize\": 12,\n",
        "    \"font.family\": \"serif\"\n",
        "})\n",
        "\n",
        "# ----- Create SHAP Summary Plot with Matplotlib Backend -----\n",
        "shap.summary_plot(\n",
        "    shap_values_test,                        # SHAP values for the selected samples\n",
        "    features=X_test_reshape,                 # Corresponding input features\n",
        "    feature_names=flattened_feature_names,   # Custom time-based feature names\n",
        "    max_display=30,                          # Top 30 features\n",
        "    plot_type=\"dot\",                         # Use dot plot\n",
        "    color_bar=True,                          # Show feature value color bar\n",
        "    plot_size=(12, 10),                      # Increase size for clarity\n",
        "    show=False                              # Don't auto-show, allows customization\n",
        "    # matplotlib=True                          # Use matplotlib backend for quality\n",
        ")\n",
        "\n",
        "# ----- Save Plot in High Resolution (PNG + Optional Vector Format) -----\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"SHAP_Summary_Plot_HighRes.pdf\", dpi=600, bbox_inches=\"tight\")      # For raster\n",
        "plt.savefig(\"SHAP_Summary_Plot_Vector.svg\", bbox_inches=\"tight\")               # For vector\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fpg9U_OjNpml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ----- Set High-Quality Plotting Parameters -----\n",
        "sns.set(style=\"whitegrid\", font_scale=1.5)\n",
        "plt.rcParams.update({\n",
        "    \"axes.titlesize\": 20,\n",
        "    \"axes.labelsize\": 16,\n",
        "    \"xtick.labelsize\": 14,\n",
        "    \"ytick.labelsize\": 14,\n",
        "    \"legend.fontsize\": 12,\n",
        "    \"font.family\": \"serif\"\n",
        "})\n",
        "\n",
        "# ----- Create SHAP Summary Plot -----\n",
        "shap.summary_plot(\n",
        "    shap_values_test,                        # SHAP values\n",
        "    features=X_test_reshape,                 # Input features\n",
        "    feature_names=flattened_feature_names,   # Custom feature names\n",
        "    max_display=30,                          # Top 30 features\n",
        "    plot_type=\"dot\",                         # Dot plot\n",
        "    color_bar=True,                          # Color bar enabled\n",
        "    plot_size=(12, 10),                      # Figure size\n",
        "    show=False                               # Defer final show\n",
        ")\n",
        "\n",
        "# ----- Post-processing: Add Title and Bold Axis Labels -----\n",
        "plt.title(\"Feature Contribution to Model Output (SHAP Summary Plot)\", fontsize=20, fontweight='bold')\n",
        "plt.xlabel(\"SHAP Value (Impact on Model Output)\", fontsize=16, fontweight='bold')\n",
        "plt.ylabel(\"Top Contributing Features\", fontsize=16, fontweight='bold')\n",
        "\n",
        "# Bold tick labels\n",
        "plt.xticks(fontsize=14, fontweight='bold')\n",
        "plt.yticks(fontsize=14, fontweight='bold')\n",
        "\n",
        "# ----- Save in High Resolution (Raster + Vector) -----\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"SHAP_Summary_Plot_HighRes.pdf\", dpi=600, bbox_inches=\"tight\")    # Raster (PDF)\n",
        "plt.savefig(\"SHAP_Summary_Plot_Vector.svg\", bbox_inches=\"tight\")             # Vector (SVG)\n",
        "\n",
        "# ----- Final Display -----\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8DheM89_SsZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set global plot style for publication\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams.update({\n",
        "    'font.family': 'serif',\n",
        "    'axes.titlesize': 18,\n",
        "    'axes.labelsize': 16,\n",
        "    'xtick.labelsize': 13,\n",
        "    'ytick.labelsize': 13,\n",
        "    'figure.dpi': 300\n",
        "})\n",
        "\n",
        "# Reconstruct predictions\n",
        "X_pred_test = autoencoder.predict(X_test)\n",
        "reconstruction_errors = np.mean(np.abs(X_pred_test - X_test), axis=(1, 2))\n",
        "feature_errors = np.abs(X_pred_test - X_test)\n",
        "feature_errors_avg = feature_errors.mean(axis=1)\n",
        "\n",
        "# Anomaly filtering\n",
        "anomaly_flags = reconstruction_errors > optimal_threshold\n",
        "anomalous_feature_errors = feature_errors_avg[anomaly_flags]\n",
        "mean_error_per_feature = anomalous_feature_errors.mean(axis=0)\n",
        "\n",
        "# DataFrame preparation\n",
        "error_df = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'MeanAnomalyError': mean_error_per_feature\n",
        "}).sort_values(by='MeanAnomalyError', ascending=True)  # sort ascending for horizontal bars\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 8))\n",
        "barplot = sns.barplot(\n",
        "    data=error_df.tail(20),  # Top 20 highest\n",
        "    x='MeanAnomalyError',\n",
        "    y='Feature',\n",
        "    palette='coolwarm'\n",
        ")\n",
        "\n",
        "# Annotate bars\n",
        "for p in barplot.patches:\n",
        "    width = p.get_width()\n",
        "    barplot.text(\n",
        "        width + 0.0005,  # Offset for readability\n",
        "        p.get_y() + p.get_height() / 2,\n",
        "        f'{width:.3f}',\n",
        "        ha='left',\n",
        "        va='center',\n",
        "        fontsize=10\n",
        "    )\n",
        "\n",
        "# Titles and labels\n",
        "plt.title(\"Top Contributing Genera/Features to Anomalous Patterns\", fontsize=18, weight='bold')\n",
        "plt.xlabel(\"Mean Absolute Error (Anomalous Sequences)\", fontsize=14, weight='bold')\n",
        "plt.ylabel(\"Genus / Feature\", fontsize=14, weight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save as high-quality figure\n",
        "plt.savefig(\"Top_Contributing_Genera.pdf\", dpi=600, bbox_inches='tight')\n",
        "# plt.savefig(\"Top_Contributing_Genera.svg\", bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f00QVUtFVJBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation of Predicted Anomalies with Clinical Markers such as neutrophil counts, temperature, and stool consistency."
      ],
      "metadata": {
        "id": "1m9MQx3yVoLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "best_autoencoder = load_model('best_model_Unsupervised.keras')\n",
        "\n",
        "# Calculate reconstruction error on all sets\n",
        "train_preds_1 = best_autoencoder.predict(X_train)\n",
        "val_preds_1 = best_autoencoder.predict(X_val)\n",
        "test_preds_1 = best_autoencoder.predict(X_test)\n",
        "\n",
        "train_mae = np.mean(np.abs(X_train - train_preds_1), axis=(1, 2))\n",
        "val_mae = np.mean(np.abs(X_val - val_preds_1), axis=(1, 2))\n",
        "test_mae = np.mean(np.abs(X_test - test_preds_1), axis=(1, 2))\n",
        "\n",
        "# Combine true labels and prediction scores for threshold optimization\n",
        "all_labels = np.concatenate([y_train, y_val, y_test])\n",
        "all_scores = np.concatenate([train_mae, val_mae, test_mae])\n",
        "\n",
        "import seaborn as sns\n",
        "# Add the predicted anomaly scores to the merged_df for correlation analysis\n",
        "# We need to map the sequence-level MAE scores back to the original time points.\n",
        "\n",
        "# Create a DataFrame for the sequence scores and their corresponding time indices\n",
        "score_df = pd.DataFrame({\n",
        "    'PatientID': [idx[0] for idx in time_idx],\n",
        "    'DayRelativeToNearestHCT': [idx[1] for idx in time_idx],\n",
        "    'AnomalyScore': all_scores\n",
        "})\n",
        "\n",
        "# Since multiple sequences can end on the same day, we might need to aggregate or take the score from the most recent sequence ending on that day. For simplicity here, we'll take the mean if multiple sequences end on the same day.\n",
        "# A more sophisticated approach would be to align the score to the specific time point in the sequence.\n",
        "# For now, let's merge based on PatientID and DayRelativeToNearestHCT, taking the first score if duplicates exist after merging, or perhaps the max/mean.\n",
        "# Let's first ensure the score_df is sorted correctly to potentially align with the original merged_df time structure if needed, though a merge is generally safer.\n",
        "\n",
        "# Sort the score_df just in case, though merge should handle matching keys.\n",
        "score_df = score_df.sort_values(['PatientID', 'DayRelativeToNearestHCT'])\n",
        "\n",
        "# Merge the anomaly scores back to the original merged_df structure\n",
        "# We need to decide how to handle the case where a day might not be the end of a sequence, or a day is the end of multiple overlapping sequences.\n",
        "# A simple approach is to merge and keep all rows from merged_df, adding the score if it exists for that (PatientID, Day). Days that weren't the end of a sequence won't have a direct score.\n",
        "# Alternatively, we can only consider the days that were the *end* of a sequence for this correlation analysis. Let's go with the latter for a direct correlation.\n",
        "\n",
        "# Filter merged_df to include only the rows corresponding to the end of sequences used in training/validation/testing\n",
        "# We can do this by merging merged_df with score_df\n",
        "correlation_df = pd.merge(merged_df, score_df, on=['PatientID', 'DayRelativeToNearestHCT'], how='inner')\n",
        "\n",
        "# Now, calculate the correlation matrix\n",
        "# We are interested in the correlation between 'AnomalyScore' and the clinical markers\n",
        "clinical_markers = ['MaxTemperature', 'NeutrophilCount'] + [col for col in correlation_df.columns if 'Consistency' in col]\n",
        "\n",
        "# Select the columns for correlation analysis\n",
        "correlation_cols = ['AnomalyScore'] + clinical_markers\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = correlation_df[correlation_cols].corr()\n",
        "\n",
        "print(\"\\nCorrelation Matrix between Predicted Anomaly Scores and Clinical Markers:\")\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Visualize the correlation matrix using a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Correlation Matrix: Anomaly Score vs. Clinical Markers', fontsize=16)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_matrix.pdf', dpi=600, format='pdf')\n",
        "plt.show()\n",
        "\n",
        "# You can also look specifically at the correlations with 'AnomalyScore'\n",
        "print(\"\\nCorrelation of Anomaly Score with Individual Clinical Markers:\")\n",
        "print(correlation_matrix['AnomalyScore'].drop('AnomalyScore'))\n",
        "\n",
        "# You might also want to visualize scatter plots for the most interesting correlations\n",
        "# Example: Scatter plot for AnomalyScore vs MaxTemperature\n",
        "if 'MaxTemperature' in clinical_markers:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(data=correlation_df, x='MaxTemperature', y='AnomalyScore', alpha=0.6)\n",
        "    plt.title('Anomaly Score vs. Max Temperature', fontsize=16)\n",
        "    plt.xlabel('Max Temperature (Scaled)', fontsize=12)\n",
        "    plt.ylabel('Anomaly Score (MAE)', fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('scatter_anomaly_temperature.pdf', dpi=600, format='pdf')\n",
        "    plt.show()\n",
        "\n",
        "# Example: Scatter plot for AnomalyScore vs NeutrophilCount\n",
        "if 'NeutrophilCount' in clinical_markers:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(data=correlation_df, x='NeutrophilCount', y='AnomalyScore', alpha=0.6)\n",
        "    plt.title('Anomaly Score vs. Neutrophil Count', fontsize=16)\n",
        "    plt.xlabel('Neutrophil Count (Scaled)', fontsize=12)\n",
        "    plt.ylabel('Anomaly Score (MAE)', fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('scatter_anomaly_neutrophil.pdf', dpi=600, format='pdf')\n",
        "    plt.show()\n",
        "\n",
        "# For one-hot encoded categorical features like Consistency, correlation is still calculated\n",
        "# but interpretation differs from continuous variables. A heatmap is usually sufficient."
      ],
      "metadata": {
        "id": "HxuOUzkYVfjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Classifier (Logistic Regression on reconstruction errors)"
      ],
      "metadata": {
        "id": "04p_WTLAn8uS"
      }
    },
    {
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assuming you have reconstruction_errors_val and y_val\n",
        "\n",
        "# Define the parameter grid for Logistic Regression\n",
        "# Ensure compatible combinations of penalty and solver\n",
        "param_grid = [\n",
        "    {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "     'penalty': ['l1', 'l2'],\n",
        "     'solver': ['liblinear', 'saga']}, # liblinear and saga support l1/l2\n",
        "    {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "     'penalty': ['elasticnet'],\n",
        "     'solver': ['saga'], # Only saga supports elasticnet\n",
        "     'l1_ratio': [0.1, 0.5, 0.9]}, # l1_ratio is needed for elasticnet\n",
        "     {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "      'penalty': [None], # Change 'none' to None (capitalized)\n",
        "      'solver': ['saga', 'lbfgs']} # saga and lbfgs support None penalty\n",
        "]\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(LogisticRegression(class_weight='balanced'), param_grid, cv=5, scoring='f1', n_jobs=-1, error_score='raise') # Using F1-score as the scoring metric\n",
        "\n",
        "# Fit the grid search to the validation data\n",
        "grid_search.fit(reconstruction_errors_val.reshape(-1, 1), y_val)\n",
        "\n",
        "# Print the best hyperparameters and best score\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best F1-score:\", grid_search.best_score_)\n",
        "\n",
        "# You can then use the best model found by grid search to evaluate on the test set\n",
        "best_clf = grid_search.best_estimator_\n",
        "predictions = best_clf.predict(reconstruction_errors_test.reshape(-1, 1))\n",
        "\n",
        "# Evaluate on the test set as you did before\n",
        "print(classification_report(y_test, predictions, zero_division=1))\n",
        "print(roc_auc_score(y_test, best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]))\n",
        "print(average_precision_score(y_test, best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "3F04_jfhRWSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict using trained classifier on test set\n",
        "predictions = best_clf.predict(reconstruction_errors_test.reshape(-1, 1))\n",
        "\n",
        "# Individual Metrics\n",
        "precision = precision_score(y_test, predictions,zero_division=1)\n",
        "recall = recall_score(y_test, predictions)\n",
        "f1 = f1_score(y_test, predictions)\n",
        "roc_auc = roc_auc_score(y_test, reconstruction_errors_test)\n",
        "pr_auc = average_precision_score(y_test, reconstruction_errors_test)\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\") # F1-score for the minority/positive class (Anomaly)\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "print(f\"PR AUC: {pr_auc:.4f}\")\n",
        "\n",
        "# Calculate weighted average F1-score\n",
        "weighted_f1 = f1_score(y_test, predictions, average='weighted',zero_division=1) #Overall summary\n",
        "print(f\"Weighted Average F1 Score: {weighted_f1:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report on Test Set (Unsupervised Fashion):\")\n",
        "print(classification_report(y_test, predictions, zero_division=1))"
      ],
      "metadata": {
        "id": "xbpFKZkSzawi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Draw PR Curve from pr_auc\n",
        "import os # Ensure os is imported\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the output directory for Logistic Regression plots\n",
        "logistic_regression_output_dir = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/BenchMark_Figures/LogisticRegression\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(logistic_regression_output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# Assuming 'reconstruction_errors' and 'true_labels' are defined as in your code\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, predictions)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.', label=f'PR Curve (AUC = {pr_auc:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "# Use the new directory variable\n",
        "plt.savefig(f\"{logistic_regression_output_dir}/PR_Curve_LR.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-eWIFQP9qmU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "logistic_regression_output_dir = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/LogisticRegression\"\n",
        "os.makedirs(logistic_regression_output_dir, exist_ok=True)\n",
        "\n",
        "# Calculate fpr and tpr using roc_curve before plotting:\n",
        "fpr, tpr, _ = roc_curve(y_test, predictions)  # Assuming 'y_test' and 'reconstruction_errors' are defined\n",
        "\n",
        "# Now you can proceed with plotting the ROC curve:\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# --- ROC ---\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.3f}')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "# Save as high-resolution PDF using the defined directory variable\n",
        "plt.savefig(f\"{logistic_regression_output_dir}/ROC_Curve_LogisticRegression.pdf\", dpi=600, bbox_inches='tight')"
      ],
      "metadata": {
        "id": "ACnn8cbgqy4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix\n",
        "import seaborn as sns\n",
        "# Calculate and assign the confusion matrix to 'cm'\n",
        "cm = confusion_matrix(y_test, predictions) # Calculate confusion matrix\n",
        "\n",
        "# Draw Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Predicted Normal', 'Predicted Anomaly'],\n",
        "            yticklabels=['Actual Normal', 'Actual Anomaly'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/LogisticRegression/PR_Curve_LogisticRegression.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ukZyWEFh3oim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras Tuner for MLP Hyperparameters"
      ],
      "metadata": {
        "id": "9E3SN7ked0WJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install keras-tuner\n",
        "# from kerastuner.tuners import RandomSearch\n",
        "\n",
        "# # Function to tune hyperparameters\n",
        "# def tune_mlp_model(hp):\n",
        "#     model = Sequential([\n",
        "#         Dense(hp.Int('neurons1', 32, 128, step=32), activation=hp.Choice('activation', ['relu', 'tanh']), input_shape=(1,)),\n",
        "#         Dense(hp.Int('neurons2', 16, 64, step=16), activation=hp.Choice('activation', ['relu', 'tanh'])),\n",
        "#         Dense(1, activation='sigmoid')\n",
        "#     ])\n",
        "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# # Define tuner\n",
        "# tuner = RandomSearch(\n",
        "#     tune_mlp_model,\n",
        "#     objective='val_accuracy',\n",
        "#     max_trials=10,\n",
        "#     executions_per_trial=1,\n",
        "#     directory='mlp_tuner',\n",
        "#     project_name='mlp_optimization'\n",
        "# )\n",
        "\n",
        "# # Search for best hyperparameters\n",
        "# tuner.search(reconstruction_errors_val.reshape(-1, 1), y_val, epochs=50, validation_split=0.2)\n",
        "\n",
        "# # Get the best hyperparameters\n",
        "# best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
        "# print(\"Best Hyperparameters:\", best_hyperparameters.values)\n"
      ],
      "metadata": {
        "id": "joF47mBHbewR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non-Linear Classifier (MLP on reconstruction errors)"
      ],
      "metadata": {
        "id": "CCrPpdBWrlqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "# Use best hyperparameters found via tuning\n",
        "input_layer = Input(shape=(1,))\n",
        "dense_1 = Dense(96, activation='relu')(input_layer)  # Update neurons1 to 96\n",
        "dense_2 = Dense(64, activation='relu')(dense_1)  # Update neurons2 to 64\n",
        "output_layer = Dense(1, activation='sigmoid')(dense_2)\n",
        "\n",
        "# Define the optimized model\n",
        "mlp_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with the same dataset but use best hyperparameters\n",
        "mlp_model.fit(reconstruction_errors_val.reshape(-1, 1), y_val, epochs=100, batch_size=16)  # Keep epochs & batch_size unchanged"
      ],
      "metadata": {
        "id": "xdAlxb8AdL9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import classification_report, roc_curve, roc_auc_score, precision_recall_curve, average_precision_score, matthews_corrcoef # Import classification_report and other necessary functions\n",
        "\n",
        "# Define the directory path where the plots will be saved\n",
        "mlp_output_dir = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/MLP\"\n",
        "\n",
        "# Create the directory if it doesn't exist. exist_ok=True prevents an error if the directory already exists.\n",
        "os.makedirs(mlp_output_dir, exist_ok=True)\n",
        "\n",
        "# Get probabilities from the trained MLP model on the test set reconstruction errors\n",
        "mlp_probs = mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Convert probabilities to binary labels (you can adjust the threshold if needed)\n",
        "threshold = 0.5\n",
        "mlp_pred_classes = (mlp_probs > threshold).astype(int)\n",
        "\n",
        "# Classification report\n",
        "print(\"MLP Classification Report (Trained on Validation, Tested on Test):\")\n",
        "print(classification_report(y_test, mlp_pred_classes, target_names=[\"Non-Dysbiotic\", \"Dysbiotic\"], zero_division=1))\n",
        "\n",
        "# Compute ROC curve and AUC score\n",
        "fpr_mlp, tpr_mlp, _ = roc_curve(y_test, mlp_probs)\n",
        "roc_auc_mlp = roc_auc_score(y_test, mlp_probs)\n",
        "print(f\"MLP ROC AUC Score: {roc_auc_mlp}\")\n",
        "\n",
        "# Compute Precision-Recall curve\n",
        "precision_mlp, recall_mlp, _ = precision_recall_curve(y_test, mlp_probs)\n",
        "average_precision_mlp = average_precision_score(y_test, mlp_probs)\n",
        "print(f\"MLP Average Precision: {average_precision_mlp}\")\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(fpr_mlp, tpr_mlp, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc_mlp:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - MLP on Reconstruction Error (Trained Val, Tested Test)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/MLP/ROC_Curve_MLP.pdf\", dpi=600, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(recall_mlp, precision_mlp, color='red', lw=2, label=f'PR curve (AP = {average_precision_mlp:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - MLP on Reconstruction Error (Trained Val, Tested Test)')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/MLP/PR_Curve_MLP.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Compute Matthews Correlation Coefficient (MCC)\n",
        "mcc_mlp = matthews_corrcoef(y_test, mlp_pred_classes)\n",
        "print(f\"MLP Matthews Correlation Coefficient (MCC): {mcc_mlp}\")"
      ],
      "metadata": {
        "id": "xC8FyH8AeZBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: PRINT PR AUC of MLP\n",
        "\n",
        "print(f\"PR AUC of MLP: {average_precision_mlp:.4f}\")\n"
      ],
      "metadata": {
        "id": "Td-Z1c8QmbRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Compute Precision, recall and F1 Score on MLP\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming y_test and mlp_pred_classes are already defined from the preceding code\n",
        "\n",
        "# Compute Precision\n",
        "precision_mlp = precision_score(y_test, mlp_pred_classes, zero_division=1)\n",
        "\n",
        "# Compute Recall\n",
        "recall_mlp = recall_score(y_test, mlp_pred_classes, zero_division=1)\n",
        "\n",
        "# Compute F1 Score\n",
        "f1_mlp = f1_score(y_test, mlp_pred_classes, zero_division=1)\n",
        "\n",
        "print(f\"MLP Precision: {precision_mlp:.4f}\")\n",
        "print(f\"MLP Recall: {recall_mlp:.4f}\")\n",
        "print(f\"MLP F1 Score: {f1_mlp:.4f}\")"
      ],
      "metadata": {
        "id": "aC0XFhqhl2_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate confusion matrix for mlp\n",
        "\n",
        "# Compute confusion matrix for MLP predictions\n",
        "cm_mlp = confusion_matrix(y_test, mlp_pred_classes)\n",
        "\n",
        "# Display confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_mlp, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Predicted Non-Dysbiotic', 'Predicted Dysbiotic'],\n",
        "            yticklabels=['Actual Non-Dysbiotic', 'Actual Dysbiotic'])\n",
        "plt.title('Confusion Matrix - MLP on Reconstruction Error (Trained Val, Tested Test)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/MLP/ConfusionMatrix_MLP.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "swzEfntj36Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Define custom scoring function\n",
        "def custom_f1_scorer(estimator, X, y):\n",
        "    anomaly_scores = estimator.decision_function(X)\n",
        "    # In One-Class SVM, lower scores mean more anomalous.\n",
        "    # We want to find a threshold such that a certain percentage of the *training* data\n",
        "    # (which is assumed to be mostly normal) is considered anomalies.\n",
        "    # The 5th percentile means the 5% lowest scores are considered anomalous.\n",
        "    # Since decision_function gives higher values for inliers, we threshold on the low side.\n",
        "    threshold = np.percentile(anomaly_scores, 5)\n",
        "    anomaly_predictions = anomaly_scores < threshold # Anomalies are below the threshold\n",
        "    # Only evaluate F1 on the actual anomalies in the test set.\n",
        "    # For RandomizedSearchCV during validation, we score against y_val\n",
        "    return f1_score(y, anomaly_predictions)\n",
        "\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "param_grid = {\n",
        "    'nu': np.linspace(0.01, 0.2, 10), # nu is an upper bound on the fraction of training errors\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.1, 1] # Kernel coefficient\n",
        "}\n",
        "\n",
        "# Set up RandomizedSearchCV\n",
        "ocsvm = OneClassSVM(kernel=\"rbf\")\n",
        "# Use make_scorer with our custom F1 function, setting greater_is_better=True (default)\n",
        "random_search = RandomizedSearchCV(ocsvm, param_grid, scoring=make_scorer(custom_f1_scorer), n_iter=10, cv=3, n_jobs=-1)\n",
        "\n",
        "# *** Calculate reconstruction errors for the training data ***\n",
        "# Assuming 'autoencoder' and 'X_train' are available from previous steps\n",
        "X_train_pred = autoencoder.predict(X_train)\n",
        "reconstruction_errors_train = np.mean(np.abs(X_train_pred - X_train), axis=(1, 2))\n",
        "\n",
        "\n",
        "# Fit search on training data's reconstruction errors\n",
        "# The y_train is used by the custom_f1_scorer during cross-validation\n",
        "# to calculate the F1 score on the validation folds within the training data.\n",
        "random_search.fit(reconstruction_errors_train.reshape(-1, 1), y_train)\n",
        "\n",
        "# Show best parameters\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Z8lJ6O-9e2q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Once Class SVM on reconstruction errors"
      ],
      "metadata": {
        "id": "Jwb4R2WVwuHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, roc_curve, auc, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler # Or StandardScaler\n",
        "\n",
        "# 1. Train OCSVM on Reconstruction Errors of Normal Data\n",
        "# Assuming 'reconstruction_errors_train' contains errors for normal training data\n",
        "\n",
        "# 1. Calculate Reconstruction Errors for Training Data\n",
        "X_train_pred = autoencoder.predict(X_train)  # Predict on training data\n",
        "reconstruction_errors_train = np.mean(np.abs(X_train_pred - X_train), axis=(1, 2))\n",
        "\n",
        "# Use best hyperparameters\n",
        "best_nu = 0.0944\n",
        "best_gamma = 0.01\n",
        "\n",
        "# Train One-Class SVM on Reconstruction Errors of Normal Data\n",
        "ocsvm = OneClassSVM(nu=best_nu, kernel=\"rbf\", gamma=best_gamma)\n",
        "ocsvm.fit(reconstruction_errors_train.reshape(-1, 1))\n",
        "\n",
        "# 2. Predict Anomaly Scores on Test Data\n",
        "anomaly_scores = ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1))\n",
        "# Lower scores indicate higher anomaly probability\n",
        "\n",
        "# 3. Thresholding for Anomaly Detection\n",
        "threshold = np.percentile(anomaly_scores, 5)  # Adjust percentile for desired sensitivity\n",
        "anomaly_predictions = anomaly_scores < threshold # Points below threshold are anomalies\n",
        "\n",
        "# 4. Evaluate Performance and Print Classification Report\n",
        "print(\"\\nOne-Class SVM Classification Report:\")\n",
        "print(classification_report(y_test, anomaly_predictions))\n",
        "\n",
        "# Define the output directory for SVM plots\n",
        "svm_output_dir = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/svm\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(svm_output_dir, exist_ok=True)\n",
        "\n",
        "# 5. Plot ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, -anomaly_scores) # Use negative scores for ROC (higher score = more anomalous)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic for One-Class SVM')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/svm/RoC_Curve_SVM.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 6. Plot PR Curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, -anomaly_scores) # Use negative scores for PR (higher score = more anomalous)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='darkorange', lw=2, label=f'PR curve (area = {pr_auc:.2f})')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for One-Class SVM')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/svm/PR_Curve_SVM.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lUhxExVGgkPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Create confusion matrix for one class SVM Classification\n",
        "from sklearn.metrics import confusion_matrix # Import confusion_matrix here\n",
        "import seaborn as sns # Import seaborn for heatmap visualization\n",
        "import matplotlib.pyplot as plt # Import matplotlib for plotting\n",
        "\n",
        "# Calculate and assign the confusion matrix for One-Class SVM to 'cm_ocsvm'\n",
        "cm_ocsvm = confusion_matrix(y_test, anomaly_predictions)\n",
        "\n",
        "# Draw Confusion Matrix for One-Class SVM\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_ocsvm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Predicted Normal', 'Predicted Anomaly'],\n",
        "            yticklabels=['Actual Normal', 'Actual Anomaly'])\n",
        "plt.title('Confusion Matrix - One-Class SVM (Scaled Errors)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/svm/confusion_matrix.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fB8KYh-f4oOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Define KNN classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Corrected hyperparameter search space\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9, 11, 15],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
        "}\n",
        "\n",
        "# Perform Randomized Search\n",
        "random_search = RandomizedSearchCV(knn, param_grid, cv=3, scoring='accuracy', n_iter=10, n_jobs=-1)\n",
        "random_search.fit(reconstruction_errors_val.reshape(-1, 1), y_val)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n"
      ],
      "metadata": {
        "id": "UL4kh0Hrg2x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Nearest Neighbors Classifier"
      ],
      "metadata": {
        "id": "XEEetiLz4vtl"
      }
    },
    {
      "source": [
        "# prompt: Add K-Nearest Neighbors Classifier\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve\n",
        "from sklearn.metrics import confusion_matrix # Import confusion_matrix\n",
        "import seaborn as sns # Import seaborn\n",
        "\n",
        "# Assuming you have already computed:\n",
        "# reconstruction_errors_val, y_val\n",
        "# reconstruction_errors_test, y_test\n",
        "\n",
        "# Train K-Nearest Neighbors Classifier\n",
        "# You might want to tune the 'n_neighbors' parameter\n",
        "#knn = KNeighborsClassifier(n_neighbors=5)\n",
        "# Use the best hyperparameters found\n",
        "knn = KNeighborsClassifier(weights='distance', n_neighbors=11, metric='minkowski', algorithm='kd_tree')\n",
        "\n",
        "# **Corrected:** Train on validation set reconstruction errors and labels\n",
        "knn.fit(reconstruction_errors_val.reshape(-1, 1), y_val)\n",
        "\n",
        "# Predict probabilities on Test Set Reconstruction Errors\n",
        "# Use the predict_proba method on the trained KNN model\n",
        "knn_probs = knn.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]\n",
        "\n",
        "# Get binary predictions using a threshold (default is 0.5 for predict_proba)\n",
        "knn_preds = (knn_probs > 0.5).astype(int)\n",
        "\n",
        "# Evaluation on Test Set\n",
        "print(\"\\nK-Nearest Neighbors Classification Report (Trained on Validation, Tested on Test):\\n\",\n",
        "      classification_report(y_test, knn_preds, digits=4, zero_division=1))\n",
        "\n",
        "# Compute ROC AUC\n",
        "# Use the probabilities for AUC calculation\n",
        "roc_auc_knn = roc_auc_score(y_test, knn_probs)\n",
        "print(\"K-Nearest Neighbors ROC AUC (Trained on Validation, Tested on Test):\", roc_auc_knn)\n",
        "\n",
        "# Compute Average Precision (PR AUC)\n",
        "# Use the probabilities for PR AUC calculation\n",
        "average_precision_knn = average_precision_score(y_test, knn_probs)\n",
        "print(\"K-Nearest Neighbors Average Precision (PR AUC) (Trained on Validation, Tested on Test):\", average_precision_knn)\n",
        "\n",
        "# Calculate Confusion Matrix for KNN\n",
        "cm_knn = confusion_matrix(y_test, knn_preds)\n",
        "\n",
        "# Draw Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_knn, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Predicted Non-Dysbiotic', 'Predicted Dysbiotic'],\n",
        "            yticklabels=['Actual Non-Dysbiotic', 'Actual Dysbiotic'])\n",
        "plt.title('Confusion Matrix - K-Nearest Neighbors on Reconstruction Error (Trained Val, Tested Test)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/K_Nearest_Neighbor/confusion_matrix.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr_knn, tpr_knn, _ = roc_curve(y_test, knn_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(fpr_knn, tpr_knn, color='green', lw=2, label=f'KNN ROC curve (AUC = {roc_auc_knn:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - K-Nearest Neighbors on Reconstruction Error (Trained Val, Tested Test)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/K_Nearest_Neighbor/KNN_ROC_curve.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "precision_knn, recall_knn, _ = precision_recall_curve(y_test, knn_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(recall_knn, precision_knn, color='purple', lw=2, label=f'KNN PR curve (AP = {average_precision_knn:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - K-Nearest Neighbors on Reconstruction Error (Trained Val, Tested Test)')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/K_Nearest_Neighbor/PR_curve.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ArBpnkjs8IPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install xgboost"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "X7DI0Rk_uLi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import cv, DMatrix\n",
        "\n",
        "# Prepare data in DMatrix format\n",
        "dtrain = DMatrix(reconstruction_errors_val.reshape(-1, 1), label=y_val)\n",
        "\n",
        "# Set parameter space\n",
        "xgb_params = {\n",
        "    'max_depth': 5,\n",
        "    'learning_rate': 0.1,\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'logloss',\n",
        "}\n",
        "\n",
        "# Perform Cross-Validation\n",
        "cv_results = cv(xgb_params, dtrain, num_boost_round=200, nfold=3, metrics=\"logloss\", early_stopping_rounds=10)\n",
        "\n",
        "# Find best number of trees\n",
        "best_n_estimators = cv_results.shape[0]\n",
        "print(f\"Best number of estimators: {best_n_estimators}\")\n"
      ],
      "metadata": {
        "id": "6tAwJldKiAPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# %%\n",
        "#!pip install xgboost\n",
        "# %%\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns  # Import seaborn\n",
        "\n",
        "# Train XGBoost on Validation Set Reconstruction Errors\n",
        "xgb = XGBClassifier(eval_metric='logloss', n_estimators=200)\n",
        "\n",
        "# **Corrected:** Train on validation set reconstruction errors\n",
        "xgb.fit(reconstruction_errors_val.reshape(-1, 1), y_val)\n",
        "\n",
        "# Predict probabilities on Test Set Reconstruction Errors\n",
        "xgb_probs = xgb.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]\n",
        "xgb_preds = (xgb_probs > 0.5).astype(int)\n",
        "\n",
        "# Evaluation on Test Set\n",
        "print(\"XGBoost Classification Report (Trained on Validation, Tested on Test):\\n\",\n",
        "      classification_report(y_test, xgb_preds, digits=4, zero_division=1))\n",
        "print(\"XGBoost ROC AUC (Trained on Validation, Tested on Test):\",\n",
        "      roc_auc_score(y_test, xgb_probs))\n",
        "print(\"XGBoost Average Precision (PR AUC) (Trained on Validation, Tested on Test):\",\n",
        "      average_precision_score(y_test, xgb_probs))\n",
        "\n",
        "# Calculate Confusion Matrix for XGBoost\n",
        "cm_xgb = confusion_matrix(y_test, xgb_preds)\n",
        "\n",
        "# Draw Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_xgb, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Predicted Non-Dysbiotic', 'Predicted Dysbiotic'],\n",
        "            yticklabels=['Actual Non-Dysbiotic', 'Actual Dysbiotic'])\n",
        "plt.title('Confusion Matrix - XGBoost on Reconstruction Error (Trained Val, Tested Test)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "#plt.savefig(\"/Final_Implementations/DynaBiome_Figures/xgboost/confusion_matrix.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_probs)\n",
        "roc_auc_xgb = roc_auc_score(y_test, xgb_probs) # Recalculate AUC using test probabilities\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(fpr_xgb, tpr_xgb, color='darkorange', lw=2, label=f'XGBoost ROC curve (AUC = {roc_auc_xgb:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic for XGBoost (Trained Val, Tested Test)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "#plt.savefig(\"/Final_Implementations/DynaBiome_Figures/xgboost/RoC_Curve.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "precision_xgb, recall_xgb, _ = precision_recall_curve(y_test, xgb_probs)\n",
        "pr_auc_xgb = average_precision_score(y_test, xgb_probs) # Recalculate PR AUC using test probabilities\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall_xgb, precision_xgb, color='darkorange', lw=2, label=f'XGBoost PR curve (AP = {pr_auc_xgb:.2f})')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for XGBoost (Trained Val, Tested Test)')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "#plt.savefig(\"/Final_Implementations/DynaBiome_Figures/xgboost/PR_Curve.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "xd3_SOz49ZRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define Random Forest classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Corrected hyperparameter search space (removed 'weights')\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200, 300],  # Number of trees\n",
        "    'max_depth': [None, 10, 20, 30],  # Maximum depth\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split\n",
        "    'min_samples_leaf': [1, 2, 5],  # Minimum samples in a leaf node\n",
        "    'max_features': ['sqrt', 'log2'],  # Number of features per tree\n",
        "    'bootstrap': [True, False]  # Whether to bootstrap samples\n",
        "}\n",
        "\n",
        "# Perform Randomized Search\n",
        "random_search = RandomizedSearchCV(rf, param_grid, cv=3, scoring='accuracy', n_iter=10, n_jobs=-1)\n",
        "random_search.fit(reconstruction_errors_val.reshape(-1, 1), y_val)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n"
      ],
      "metadata": {
        "id": "1SNYWFNmnnWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Classifier"
      ],
      "metadata": {
        "id": "E26cqMxxQz13"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve\n",
        "from sklearn.metrics import confusion_matrix # Import confusion_matrix\n",
        "import seaborn as sns # Import seaborn\n",
        "\n",
        "# Assuming you have already computed reconstruction_errors_val, y_val,\n",
        "# reconstruction_errors_test, and y_test from previous steps.\n",
        "\n",
        "# Train Random Forest on Validation Set Reconstruction Errors\n",
        "# You might want to tune the 'n_estimators' and other parameters\n",
        "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "# Use best hyperparameters found\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=5,\n",
        "    max_features='log2',\n",
        "    max_depth=10,\n",
        "    bootstrap=False,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf.fit(reconstruction_errors_val.reshape(-1, 1), y_val) # Train on validation errors and labels\n",
        "\n",
        "# Predict probabilities on Test Set Reconstruction Errors\n",
        "rf_probs = rf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]\n",
        "rf_preds = (rf_probs > 0.5).astype(int) # Use a threshold (e.g., 0.5) to get binary predictions\n",
        "\n",
        "# Evaluation on Test Set\n",
        "print(\"Random Forest Classification Report (Trained on Validation, Tested on Test):\\n\",\n",
        "      classification_report(y_test, rf_preds, digits=4, zero_division=1))\n",
        "print(\"Random Forest ROC AUC (Trained on Validation, Tested on Test):\",\n",
        "      roc_auc_score(y_test, rf_probs))\n",
        "print(\"Random Forest Average Precision (PR AUC) (Trained on Validation, Tested on Test):\",\n",
        "      average_precision_score(y_test, rf_probs))\n",
        "\n",
        "# Calculate Confusion Matrix for Random Forest\n",
        "cm_rf = confusion_matrix(y_test, rf_preds)\n",
        "\n",
        "# Draw Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_rf, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Predicted Non-Dysbiotic', 'Predicted Dysbiotic'],\n",
        "            yticklabels=['Actual Non-Dysbiotic', 'Actual Dysbiotic'])\n",
        "plt.title('Confusion Matrix - Random Forest on Reconstruction Error (Trained Val, Tested Test)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/RF/confusion_matrix.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs)\n",
        "roc_auc_rf = roc_auc_score(y_test, rf_probs) # Recalculate ROC AUC using test probabilities\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(fpr_rf, tpr_rf, color='darkorange', lw=2, label=f'Random Forest ROC curve (AUC = {roc_auc_rf:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic for Random Forest (Trained Val, Tested Test)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/RF/RoC_Curve.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "precision_rf, recall_rf, _ = precision_recall_curve(y_test, rf_probs)\n",
        "average_precision_rf = average_precision_score(y_test, rf_probs) # Recalculate PR AUC using test probabilities\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall_rf, precision_rf, color='darkorange', lw=2, label=f'Random Forest PR curve (AP = {average_precision_rf:.2f})')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Random Forest (Trained Val, Tested Test)')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/RF/PR_Curve.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "byFUIBiS62Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ensemble-learning\"></a>\n",
        "# Ensemble Learning on the Classifiers"
      ],
      "metadata": {
        "id": "FbicO3dJ1oEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Averaged Probabilities (Soft Voting)"
      ],
      "metadata": {
        "id": "ELCSC1Q6zeKk"
      }
    },
    {
      "source": [
        "# Assuming the following trained models are available from your notebook:\n",
        "# best_clf (Logistic Regression - trained on reconstruction_errors_val)\n",
        "# mlp_model (Keras MLP - trained on reconstruction_errors_val)\n",
        "# ocsvm (One-Class SVM - trained on reconstruction_errors_train, decision_function on test)\n",
        "# knn (K-Nearest Neighbors - trained on reconstruction_errors_val)\n",
        "# xgb (XGBoost - trained on reconstruction_errors_val)\n",
        "# rf (Random Forest - trained on reconstruction_errors_val)\n",
        "\n",
        "# Ensure reconstruction_errors_test is available and reshaped if needed for models expecting (n_samples, 1) input\n",
        "reconstruction_errors_test_reshaped = reconstruction_errors_test.reshape(-1, 1)\n",
        "\n",
        "# 1. Logistic Regression Probabilities\n",
        "lr_probs = best_clf.predict_proba(reconstruction_errors_test_reshaped)[:, 1]\n",
        "\n",
        "# 2. Keras MLP Probabilities\n",
        "# Make sure the MLP model is loaded if necessary\n",
        "# from tensorflow.keras.models import load_model\n",
        "# mlp_model = load_model(\"your_mlp_model.h5\") # Load if not already in memory\n",
        "mlp_probs = mlp_model.predict(reconstruction_errors_test_reshaped).flatten()\n",
        "\n",
        "# 3. KNN Probabilities\n",
        "knn_probs = knn.predict_proba(reconstruction_errors_test_reshaped)[:, 1]\n",
        "\n",
        "# 4. XGBoost Probabilities\n",
        "xgb_probs = xgb.predict_proba(reconstruction_errors_test_reshaped)[:, 1]\n",
        "\n",
        "# 5. Random Forest Probabilities\n",
        "rf_probs = rf.predict_proba(reconstruction_errors_test_reshaped)[:, 1]\n",
        "\n",
        "# 6. One-Class SVM Scores\n",
        "# OCSVM's decision_function gives a score. Lower scores are more anomalous.\n",
        "# For soft voting, we need a probability-like score where higher indicates anomaly.\n",
        "# We can use the negative of the decision function.\n",
        "ocsvm_scores = -ocsvm.decision_function(reconstruction_errors_test_reshaped).flatten()\n",
        "\n",
        "# You might consider scaling the OCSVM scores to be on a similar scale as probabilities (0-1)\n",
        "# For instance, using MinMaxScaler if needed, but it's not strictly necessary for simple averaging.\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# scaler = MinMaxScaler()\n",
        "# ocsvm_scaled_scores = scaler.fit_transform(ocsvm_scores.reshape(-1, 1)).flatten()\n",
        "# We'll use the raw negative scores for now in the ensemble calculation."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "L4hmrv0kuZIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "# Combine probabilities (excluding OCSVM for simple averaging with probability-based models)\n",
        "# If you want to include OCSVM, you would need a way to convert its score to a probability or\n",
        "# use a different ensembling approach that handles scores and probabilities.\n",
        "# Let's average the probabilities from LR, MLP, KNN, XGBoost, and RF for a start.\n",
        "averaged_probs = np.mean([lr_probs, mlp_probs, knn_probs, xgb_probs, rf_probs], axis=0)\n",
        "\n",
        "# You can set a threshold on the averaged probabilities to get binary predictions\n",
        "ensemble_predictions = (averaged_probs > 0.5).astype(int)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "YjdjozwEub5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve\n",
        "import seaborn as sns\n",
        "\n",
        "# Classification Report\n",
        "print(\"Ensemble (Averaged Probabilities) Classification Report on Test Set:\")\n",
        "print(classification_report(y_test, ensemble_predictions, target_names=[\"Non-Dysbiotic\", \"Dysbiotic\"], zero_division=1))\n",
        "\n",
        "# Compute ROC AUC\n",
        "roc_auc_ensemble = roc_auc_score(y_test, averaged_probs)\n",
        "print(f\"Ensemble ROC AUC Score: {roc_auc_ensemble:.4f}\")\n",
        "\n",
        "# Compute Average Precision (PR AUC)\n",
        "average_precision_ensemble = average_precision_score(y_test, averaged_probs)\n",
        "print(f\"Ensemble Average Precision (PR AUC): {average_precision_ensemble:.4f}\")\n",
        "\n",
        "# Calculate Confusion Matrix\n",
        "cm_ensemble = confusion_matrix(y_test, ensemble_predictions)\n",
        "\n",
        "# Draw Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_ensemble, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Predicted Non-Dysbiotic', 'Predicted Dysbiotic'],\n",
        "            yticklabels=['Actual Non-Dysbiotic', 'Actual Dysbiotic'])\n",
        "plt.title('Confusion Matrix - Ensemble (Averaged Probabilities)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Avg/confusion_matrix.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr_ensemble, tpr_ensemble, _ = roc_curve(y_test, averaged_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(fpr_ensemble, tpr_ensemble, color='blue', lw=2, label=f'Ensemble ROC curve (AUC = {roc_auc_ensemble:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Ensemble (Averaged Probabilities)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Avg/RoC_Curve.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "precision_ensemble, recall_ensemble, _ = precision_recall_curve(y_test, averaged_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(recall_ensemble, precision_ensemble, color='red', lw=2, label=f'Ensemble PR curve (AP = {average_precision_ensemble:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - Ensemble (Averaged Probabilities)')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Avg/PR_Curve.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UQL043FSudl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weighted Mechanism (Weighted Averaging) (Soft Voting)\n",
        "\n"
      ],
      "metadata": {
        "id": "H_ptCBawz0rg"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "# Assuming you have the scikit-learn versions of your trained models:\n",
        "# best_clf (Logistic Regression)\n",
        "# sklearn_mlp (if you train a scikit-learn MLP)\n",
        "# knn (K-Nearest Neighbors)\n",
        "# xgb (XGBoost)\n",
        "# rf (Random Forest)\n",
        "\n",
        "# For One-Class SVM, you would typically not include it in a standard soft voting classifier\n",
        "# because its decision_function is not directly comparable to probability outputs of others.\n",
        "# If you want to include its influence, you might use hard voting or a custom ensembling approach.\n",
        "\n",
        "estimators_sklearn = [\n",
        "    ('lr', best_clf),\n",
        "    ('knn', knn),\n",
        "    ('xgb', xgb),\n",
        "    ('rf', rf)\n",
        "    # Add sklearn_mlp if available\n",
        "    # ('mlp', sklearn_mlp)\n",
        "]\n",
        "\n",
        "# Create the Voting Classifier with soft voting\n",
        "voting_clf = VotingClassifier(estimators=estimators_sklearn, voting='soft', weights=[1, 1, 1, 1]) # Adjust weights as needed\n",
        "\n",
        "# Train the Voting Classifier (This trains the individual models if not already trained, but\n",
        "# since yours are trained, it essentially just sets up the combination)\n",
        "# However, typically, you train the individual models first and then use the VotingClassifier\n",
        "# to combine their predictions without retraining.\n",
        "# To use the already trained models' predictions directly in VotingClassifier for evaluation:\n",
        "# You would fit the VotingClassifier on a small dummy dataset just to enable prediction,\n",
        "# or more correctly, collect the predictions and combine them manually as shown in the previous steps.\n",
        "\n",
        "# Let's stick to the manual averaging of probabilities as it directly uses your existing trained models' outputs.\n",
        "# If you were to use VotingClassifier's fit method, it would re-train the estimators,\n",
        "# which is not what you want if you've already tuned and trained them on the validation data.\n",
        "\n",
        "# The manual averaging method shown above is the correct way to ensemble\n",
        "# the predictions of your already trained models on the test set."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7r4JiM2sukd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Example of weighted averaging (adjust weights based on your analysis)\n",
        "# You would need to determine these weights based on the performance of each model on the validation set (e.g., F1-score, ROC AUC).\n",
        "# For example, if XGBoost had the highest F1 on validation, give it a higher weight.\n",
        "weights = [1.2, 0.8, 1.5, 1.0, 1.1] # Example weights for LR, MLP, KNN, XGBoost, RF\n",
        "weighted_averaged_probs = np.average([lr_probs, mlp_probs, knn_probs, xgb_probs, rf_probs], axis=0, weights=weights)\n",
        "\n",
        "weighted_ensemble_predictions = (weighted_averaged_probs > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the weighted ensemble similarly to the unweighted one.\n",
        "print(\"\\nWeighted Ensemble Classification Report on Test Set:\")\n",
        "print(classification_report(y_test, weighted_ensemble_predictions, target_names=[\"Non-Dysbiotic\", \"Dysbiotic\"], zero_division=1))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "__-DfYUSuokG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: calculate and plot metrics for the weighted ensemble\n",
        "\n",
        "# Compute ROC AUC for weighted ensemble\n",
        "roc_auc_weighted_ensemble = roc_auc_score(y_test, weighted_averaged_probs)\n",
        "print(f\"Weighted Ensemble ROC AUC Score: {roc_auc_weighted_ensemble:.4f}\")\n",
        "\n",
        "# Compute Average Precision (PR AUC) for weighted ensemble\n",
        "average_precision_weighted_ensemble = average_precision_score(y_test, weighted_averaged_probs)\n",
        "print(f\"Weighted Ensemble Average Precision (PR AUC): {average_precision_weighted_ensemble:.4f}\")\n",
        "\n",
        "# Calculate Confusion Matrix for weighted ensemble\n",
        "cm_weighted_ensemble = confusion_matrix(y_test, weighted_ensemble_predictions)\n",
        "\n",
        "# Define output directory for Weighted Ensemble plots\n",
        "weighted_ensemble_output_dir = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Weighted\"\n",
        "os.makedirs(weighted_ensemble_output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# Draw Confusion Matrix for weighted ensemble\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_weighted_ensemble, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Predicted Non-Dysbiotic', 'Predicted Dysbiotic'],\n",
        "            yticklabels=['Actual Non-Dysbiotic', 'Actual Dysbiotic'])\n",
        "plt.title('Confusion Matrix - Weighted Ensemble')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.savefig(f\"{weighted_ensemble_output_dir}/confusion_matrix.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve for weighted ensemble\n",
        "fpr_weighted_ensemble, tpr_weighted_ensemble, _ = roc_curve(y_test, weighted_averaged_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(fpr_weighted_ensemble, tpr_weighted_ensemble, color='green', lw=2, label=f'Weighted Ensemble ROC curve (AUC = {roc_auc_weighted_ensemble:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Weighted Ensemble')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.savefig(f\"{weighted_ensemble_output_dir}/RoC_Curve.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot Precision-Recall curve for weighted ensemble\n",
        "precision_weighted_ensemble, recall_weighted_ensemble, _ = precision_recall_curve(y_test, weighted_averaged_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(recall_weighted_ensemble, precision_weighted_ensemble, color='purple', lw=2, label=f'Weighted Ensemble PR curve (AP = {average_precision_weighted_ensemble:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - Weighted Ensemble')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "plt.savefig(f\"{weighted_ensemble_output_dir}/PR_Curve.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QGz8AAgeO0Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Print RoC (AUC) & PR (AUC) of weighted esemble\n",
        "\n",
        "print(f\"RoC (AUC) of Weighted Ensemble: {roc_auc_weighted_ensemble:.4f}\")\n",
        "print(f\"PR (AUC) of Weighted Ensemble: {average_precision_weighted_ensemble:.4f}\")\n"
      ],
      "metadata": {
        "id": "kuwzwPB3oaHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Ensembling: Stacking"
      ],
      "metadata": {
        "id": "4l8WY6sI1TyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stacking with LogisticRegression"
      ],
      "metadata": {
        "id": "C7ycWbTw0KSy"
      }
    },
    {
      "source": [
        "# Generate Out-of-Fold Predictions on the Validation Set\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have reconstruction_errors_val and y_val\n",
        "\n",
        "# Number of folds for cross-validation\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize arrays to store out-of-fold predictions\n",
        "# Each column will be the predictions from one base model\n",
        "oof_preds = np.zeros((reconstruction_errors_val.shape[0], 5)) # 5 columns for LR, MLP, KNN, XGB, RF\n",
        "\n",
        "# List of base models (excluding OCSVM for now as its output is a score, not probability)\n",
        "base_models = [\n",
        "    ('lr', best_clf),\n",
        "    ('mlp', mlp_model), # We'll handle Keras prediction separately\n",
        "    ('knn', knn),\n",
        "    ('xgb', xgb),\n",
        "    ('rf', rf)\n",
        "]\n",
        "\n",
        "# Iterate through folds and generate out-of-fold predictions\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(reconstruction_errors_val.reshape(-1, 1), y_val)):\n",
        "    print(f\"Processing Fold {fold + 1}/{n_splits}\")\n",
        "\n",
        "    X_train_fold, X_val_fold = reconstruction_errors_val[train_index].reshape(-1, 1), reconstruction_errors_val[val_index].reshape(-1, 1)\n",
        "    y_train_fold, y_val_fold = y_val[train_index], y_val[val_index]\n",
        "\n",
        "    for i, (name, model) in enumerate(base_models):\n",
        "        if name == 'mlp':\n",
        "            # For Keras model, train on the fold data\n",
        "            # Need to clone and retrain the model for each fold or save/load weights\n",
        "            # A simpler approach for demonstration: assuming the trained mlp_model is sufficient\n",
        "            # and we generate predictions on the val_index for this fold.\n",
        "            # In a proper implementation, you would train a new Keras model on X_train_fold\n",
        "            # and predict on X_val_fold.\n",
        "            fold_mlp_preds = model.predict(X_val_fold).flatten()\n",
        "            oof_preds[val_index, i] = fold_mlp_preds\n",
        "        else:\n",
        "            # Train a clone of the scikit-learn model on the fold data\n",
        "            from sklearn.base import clone\n",
        "            cloned_model = clone(model)\n",
        "            cloned_model.fit(X_train_fold, y_train_fold)\n",
        "            oof_preds[val_index, i] = cloned_model.predict_proba(X_val_fold)[:, 1]\n",
        "\n",
        "# oof_preds now contains the out-of-fold predictions for each base model on the validation set"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "KJgZw3eIu1u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Combine test predictions from base models\n",
        "test_preds = np.vstack([lr_probs, mlp_probs, knn_probs, xgb_probs, rf_probs]).T\n",
        "\n",
        "# test_preds now has shape (n_test_samples, n_base_models)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "PIiF7f3Zu6S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the meta-model\n",
        "meta_model = LogisticRegression()\n",
        "\n",
        "# Train the meta-model on the out-of-fold predictions\n",
        "meta_model.fit(oof_preds, y_val)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LBSj-DPMu8Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Make final predictions on the test data using the meta-model\n",
        "stacked_predictions_probs = meta_model.predict_proba(test_preds)[:, 1]\n",
        "\n",
        "# Convert probabilities to binary predictions\n",
        "stacked_predictions = (stacked_predictions_probs > 0.5).astype(int)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1im3Rc6Ou-Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve\n",
        "import seaborn as sns\n",
        "\n",
        "# Classification Report\n",
        "print(\"Stacked Ensemble Classification Report on Test Set:\")\n",
        "print(classification_report(y_test, stacked_predictions, target_names=[\"Non-Dysbiotic\", \"Dysbiotic\"], zero_division=1))\n",
        "\n",
        "# Compute ROC AUC\n",
        "roc_auc_stacked = roc_auc_score(y_test, stacked_predictions_probs)\n",
        "print(f\"Stacked Ensemble ROC AUC Score: {roc_auc_stacked:.4f}\")\n",
        "\n",
        "# Compute Average Precision (PR AUC)\n",
        "average_precision_stacked = average_precision_score(y_test, stacked_predictions_probs)\n",
        "print(f\"Stacked Ensemble Average Precision (PR AUC): {average_precision_stacked:.4f}\")\n",
        "\n",
        "# Calculate Confusion Matrix\n",
        "cm_stacked = confusion_matrix(y_test, stacked_predictions)\n",
        "\n",
        "# Draw Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_stacked, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Predicted Non-Dysbiotic', 'Predicted Dysbiotic'],\n",
        "            yticklabels=['Actual Non-Dysbiotic', 'Actual Anomaly'])\n",
        "plt.title('Confusion Matrix - Stacked Ensemble')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr_stacked, tpr_stacked, _ = roc_curve(y_test, stacked_predictions_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(fpr_stacked, tpr_stacked, color='blue', lw=2, label=f'Stacked ROC curve (AUC = {roc_auc_stacked:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Stacked Ensemble')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Stacking/RoC_Curve.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "precision_stacked, recall_stacked, _ = precision_recall_curve(y_test, stacked_predictions_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(recall_stacked, precision_stacked, color='red', lw=2, label=f'Stacked PR curve (AP = {average_precision_stacked:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - Stacked Ensemble')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Stacking/PR_Curve.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rRjCF3hbu_1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stacking with XGBoost Meta-Model"
      ],
      "metadata": {
        "id": "lsF_TtYa0-Tj"
      }
    },
    {
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Define the XGBoost meta-model\n",
        "# You might want to tune the hyperparameters of the meta-model as well\n",
        "meta_model_xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Train the meta-model on the out-of-fold predictions\n",
        "meta_model_xgb.fit(oof_preds, y_val)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "yxNFDHUm02ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Make final predictions on the test data using the XGBoost meta-model\n",
        "stacked_predictions_probs_xgb = meta_model_xgb.predict_proba(test_preds)[:, 1]\n",
        "\n",
        "# Convert probabilities to binary predictions\n",
        "stacked_predictions_xgb = (stacked_predictions_probs_xgb > 0.5).astype(int)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "WpMaEaQS04tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve\n",
        "import seaborn as sns\n",
        "\n",
        "# Classification Report\n",
        "print(\"Stacked Ensemble (XGBoost Meta-Model) Classification Report on Test Set:\")\n",
        "print(classification_report(y_test, stacked_predictions_xgb, target_names=[\"Non-Dysbiotic\", \"Dysbiotic\"], zero_division=1))\n",
        "\n",
        "# Compute ROC AUC\n",
        "roc_auc_stacked_xgb = roc_auc_score(y_test, stacked_predictions_probs_xgb)\n",
        "print(f\"Stacked Ensemble (XGBoost Meta-Model) ROC AUC Score: {roc_auc_stacked_xgb:.4f}\")\n",
        "\n",
        "# Compute Average Precision (PR AUC)\n",
        "average_precision_stacked_xgb = average_precision_score(y_test, stacked_predictions_probs_xgb)\n",
        "print(f\"Stacked Ensemble (XGBoost Meta-Model) Average Precision (PR AUC): {average_precision_stacked_xgb:.4f}\")\n",
        "\n",
        "# Calculate Confusion Matrix\n",
        "cm_stacked_xgb = confusion_matrix(y_test, stacked_predictions_xgb)\n",
        "\n",
        "# Draw Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_stacked_xgb, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Predicted Non-Dysbiotic', 'Predicted Dysbiotic'],\n",
        "            yticklabels=['Actual Non-Dysbiotic', 'Actual Anomaly'])\n",
        "plt.title('Confusion Matrix - Stacked Ensemble (XGBoost Meta-Model)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Stacking_XGB/confusion_matrix.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr_stacked_xgb, tpr_stacked_xgb, _ = roc_curve(y_test, stacked_predictions_probs_xgb)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(fpr_stacked_xgb, tpr_stacked_xgb, color='blue', lw=2, label=f'Stacked (XGBoost) ROC curve (AUC = {roc_auc_stacked_xgb:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Stacked Ensemble (XGBoost Meta-Model)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Stacking_XGB/RoC_Curve.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "precision_stacked_xgb, recall_stacked_xgb, _ = precision_recall_curve(y_test, stacked_predictions_probs_xgb)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(recall_stacked_xgb, precision_stacked_xgb, color='red', lw=2, label=f'Stacked (XGBoost) PR curve (AP = {average_precision_stacked_xgb:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - Stacked Ensemble (XGBoost Meta-Model)')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "#plt.savefig(\"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Ensemble_Stacking_XGB/PR_Curve.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gBZZN9s705eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: prompt: There is too much overlap between the lines in the ROC (AUC) and PR(AUC). Fix it\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# --- B. Combined ROC and PR Plotting ---\n",
        "\n",
        "# Store ROC and PR curve data for each model\n",
        "roc_curves_data = {}\n",
        "pr_curves_data = {}\n",
        "\n",
        "# Function to collect ROC and PR data\n",
        "def collect_curve_data(model_name, y_true, y_probs_or_scores, color, linestyle='-'):\n",
        "    \"\"\"Collects ROC and PR curve data for a given model's scores/probabilities.\"\"\"\n",
        "    # For ROC curve, higher score/prob means higher true positive rate at lower false positive rate\n",
        "    # For PR curve, higher score/prob means higher precision at higher recall\n",
        "    # sklearn's roc_curve and precision_recall_curve work correctly with scores directly.\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_probs_or_scores)\n",
        "    roc_auc = roc_auc_score(y_true, y_probs_or_scores)\n",
        "    roc_curves_data[model_name] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc, 'color': color, 'linestyle': linestyle}\n",
        "\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_probs_or_scores)\n",
        "    pr_auc = average_precision_score(y_true, y_probs_or_scores)\n",
        "    pr_curves_data[model_name] = {'precision': precision, 'recall': recall, 'auc': pr_auc, 'color': color, 'linestyle': linestyle}\n",
        "\n",
        "# Collect data for Original Model (LSTM-AE with optimal threshold)\n",
        "# Use reconstruction errors as the score directly. Higher error -> more likely anomaly.\n",
        "collect_curve_data(\n",
        "    'LSTM-AE (Reconstruction Error)',\n",
        "    y_test,\n",
        "    reconstruction_errors_test,\n",
        "    'darkorange',\n",
        "    '-'\n",
        ")\n",
        "\n",
        "# Collect data for Individual Classifiers (assuming probabilities are available)\n",
        "# Use predict_proba[:, 1] for standard classifiers\n",
        "# For OCSVM, use negative scores as the \"probability\" or score for AUC\n",
        "collect_curve_data('Logistic Regression', y_test, best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'blue', '--')\n",
        "collect_curve_data('MLP', y_test, mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten(), 'green', '--')\n",
        "# For OCSVM, use negative decision_function scores. Lower scores are anomalous, so negative makes higher values anomalous.\n",
        "collect_curve_data('One-Class SVM', y_test, -ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1)).flatten(), 'red', '--')\n",
        "collect_curve_data('K-Nearest Neighbors', y_test, knn.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'purple', '--')\n",
        "collect_curve_data('XGBoost', y_test, xgb.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'brown', '--')\n",
        "collect_curve_data('Random Forest', y_test, rf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'pink', '--')\n",
        "\n",
        "\n",
        "# Collect data for Ensemble Methods (using the averaged/stacked probabilities)\n",
        "collect_curve_data('Averaged Probabilities Ensemble', y_test, averaged_probs, 'cyan', '-')\n",
        "collect_curve_data('Weighted Ensemble', y_test, weighted_averaged_probs, 'magenta', '-')\n",
        "# Use the stacked ensemble probabilities as scores\n",
        "collect_curve_data('Stacked Ensemble (LR Meta)', y_test, meta_model.predict_proba(test_preds)[:, 1], 'gray', '-')\n",
        "collect_curve_data('Stacked Ensemble (XGB Meta)', y_test, meta_model_xgb.predict_proba(test_preds)[:, 1], 'olive', '-')\n",
        "\n",
        "\n",
        "# --- Plotting ---\n",
        "\n",
        "output_dir_evaluation_plots = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Combined_Evaluation_Plots/\"\n",
        "os.makedirs(output_dir_evaluation_plots, exist_ok=True)\n",
        "\n",
        "# Figure 1: ROC Curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "for model_name, data in roc_curves_data.items():\n",
        "    plt.plot(data['fpr'], data['tpr'], color=data['color'], linestyle=data['linestyle'],\n",
        "             label=f'{model_name} (AUC = {data[\"auc\"]:.3f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=1)  # Diagonal line\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
        "plt.title(\"Receiver Operating Characteristic (ROC) Curves\", fontsize=14)\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{output_dir_evaluation_plots}/Combined_ROC_Curves.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Figure 2: PR Curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "for model_name, data in pr_curves_data.items():\n",
        "    plt.plot(data['recall'], data['precision'], color=data['color'], linestyle=data['linestyle'],\n",
        "             label=f'{model_name} (AP = {data[\"auc\"]:.3f})')\n",
        "\n",
        "plt.xlabel(\"Recall\", fontsize=12)\n",
        "plt.ylabel(\"Precision\", fontsize=12)\n",
        "plt.title(\"Precision-Recall (PR) Curves\", fontsize=14)\n",
        "plt.legend(loc=\"lower left\", fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.ylim([0.0, 1.05]) # Standard practice for PR curves\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{output_dir_evaluation_plots}/Combined_PR_Curves.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VNOy1nJcqbY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RoC (AUC) as original vs All Classifiers except ensemble learning"
      ],
      "metadata": {
        "id": "nyLgGLZwdvlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# --- Plotting ROC Curve for Original Model vs. Individual Classifiers (Excluding Ensemble) ---\n",
        "\n",
        "# Define the output directory\n",
        "output_dir_comparison_plots = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Comparison_Plots/\"\n",
        "os.makedirs(output_dir_comparison_plots, exist_ok=True)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Plot ROC Curve for the Original Model (LSTM-AE Reconstruction Error)\n",
        "# Use reconstruction errors as the score directly. Higher error -> more likely anomaly.\n",
        "fpr_original, tpr_original, _ = roc_curve(y_test, reconstruction_errors_test)\n",
        "roc_auc_original = roc_auc_score(y_test, reconstruction_errors_test)\n",
        "plt.plot(fpr_original, tpr_original, color='darkorange', lw=2, label=f'LSTM-AE (Reconstruction Error) (AUC = {roc_auc_original:.3f})')\n",
        "\n",
        "\n",
        "# Plot ROC Curves for Individual Classifiers (trained on reconstruction errors)\n",
        "# Use predict_proba[:, 1] for standard classifiers. For OCSVM, use negative scores.\n",
        "\n",
        "# Logistic Regression\n",
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1])\n",
        "roc_auc_lr = roc_auc_score(y_test, best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1])\n",
        "plt.plot(fpr_lr, tpr_lr, color='blue', linestyle='--', lw=2, label=f'Logistic Regression (AUC = {roc_auc_lr:.3f})')\n",
        "\n",
        "# MLP\n",
        "mlp_probs_test = mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten()\n",
        "fpr_mlp, tpr_mlp, _ = roc_curve(y_test, mlp_probs_test)\n",
        "roc_auc_mlp = roc_auc_score(y_test, mlp_probs_test)\n",
        "plt.plot(fpr_mlp, tpr_mlp, color='green', linestyle='--', lw=2, label=f'MLP (AUC = {roc_auc_mlp:.3f})')\n",
        "\n",
        "# One-Class SVM\n",
        "ocsvm_scores_test = -ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1)).flatten()\n",
        "fpr_ocsvm, tpr_ocsvm, _ = roc_curve(y_test, ocsvm_scores_test)\n",
        "roc_auc_ocsvm = roc_auc_score(y_test, ocsvm_scores_test)\n",
        "plt.plot(fpr_ocsvm, tpr_ocsvm, color='red', linestyle='--', lw=2, label=f'One-Class SVM (AUC = {roc_auc_ocsvm:.3f})')\n",
        "\n",
        "# K-Nearest Neighbors\n",
        "knn_probs_test = knn.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]\n",
        "fpr_knn, tpr_knn, _ = roc_curve(y_test, knn_probs_test)\n",
        "roc_auc_knn = roc_auc_score(y_test, knn_probs_test)\n",
        "plt.plot(fpr_knn, tpr_knn, color='purple', linestyle='--', lw=2, label=f'K-Nearest Neighbors (AUC = {roc_auc_knn:.3f})')\n",
        "\n",
        "# XGBoost\n",
        "xgb_probs_test = xgb.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]\n",
        "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_probs_test)\n",
        "roc_auc_xgb = roc_auc_score(y_test, xgb_probs_test)\n",
        "plt.plot(fpr_xgb, tpr_xgb, color='brown', linestyle='--', lw=2, label=f'XGBoost (AUC = {roc_auc_xgb:.3f})')\n",
        "\n",
        "# Random Forest\n",
        "rf_probs_test = rf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs_test)\n",
        "roc_auc_rf = roc_auc_score(y_test, rf_probs_test)\n",
        "plt.plot(fpr_rf, tpr_rf, color='pink', linestyle='--', lw=2, label=f'Random Forest (AUC = {roc_auc_rf:.3f})')\n",
        "\n",
        "\n",
        "# Plot the diagonal reference line\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
        "plt.title(\"ROC Curves: LSTM-AE vs. Individual Classifiers\", fontsize=14)\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{output_dir_comparison_plots}/ROC_Curve_Original_vs_Individual_Classifiers.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LXsO9A_jvF_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RoC (AUC) as original vs all ensemble learning only"
      ],
      "metadata": {
        "id": "XFcO4axBd02D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate RoC (AUC) as original vs all ensemble learning only\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# --- Plotting ROC Curve for Original Model vs. Ensemble Learning ---\n",
        "\n",
        "# Define the output directory for comparison plots if not already defined\n",
        "output_dir_comparison_plots = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Comparison_Plots/\"\n",
        "os.makedirs(output_dir_comparison_plots, exist_ok=True)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Plot ROC Curve for the Original Model (LSTM-AE Reconstruction Error)\n",
        "# Use reconstruction errors as the score directly. Higher error -> more likely anomaly.\n",
        "fpr_original, tpr_original, _ = roc_curve(y_test, reconstruction_errors_test)\n",
        "roc_auc_original = roc_auc_score(y_test, reconstruction_errors_test)\n",
        "plt.plot(fpr_original, tpr_original, color='darkorange', lw=2, label=f'LSTM-AE (Reconstruction Error) (AUC = {roc_auc_original:.3f})')\n",
        "\n",
        "\n",
        "# Plot ROC Curves for Ensemble Models\n",
        "\n",
        "# Averaged Probabilities Ensemble\n",
        "fpr_avg_ensemble, tpr_avg_ensemble, _ = roc_curve(y_test, averaged_probs)\n",
        "roc_auc_avg_ensemble = roc_auc_score(y_test, averaged_probs)\n",
        "plt.plot(fpr_avg_ensemble, tpr_avg_ensemble, color='cyan', linestyle='-', lw=2, label=f'Averaged Probabilities Ensemble (AUC = {roc_auc_avg_ensemble:.3f})')\n",
        "\n",
        "# Weighted Ensemble\n",
        "fpr_weighted_ensemble, tpr_weighted_ensemble, _ = roc_curve(y_test, weighted_averaged_probs)\n",
        "roc_auc_weighted_ensemble = roc_auc_score(y_test, weighted_averaged_probs)\n",
        "plt.plot(fpr_weighted_ensemble, tpr_weighted_ensemble, color='magenta', linestyle='-', lw=2, label=f'Weighted Ensemble (AUC = {roc_auc_weighted_ensemble:.3f})')\n",
        "\n",
        "# Stacked Ensemble (Logistic Regression Meta-Model)\n",
        "# Assuming test_preds contains the stacked inputs for test data\n",
        "stacked_predictions_probs_lr_meta = meta_model.predict_proba(test_preds)[:, 1]\n",
        "fpr_stacked_lr, tpr_stacked_lr, _ = roc_curve(y_test, stacked_predictions_probs_lr_meta)\n",
        "roc_auc_stacked_lr = roc_auc_score(y_test, stacked_predictions_probs_lr_meta)\n",
        "plt.plot(fpr_stacked_lr, tpr_stacked_lr, color='gray', linestyle='-', lw=2, label=f'Stacked Ensemble (LR Meta) (AUC = {roc_auc_stacked_lr:.3f})')\n",
        "\n",
        "# Stacked Ensemble (XGBoost Meta-Model)\n",
        "# Assuming test_preds contains the stacked inputs for test data\n",
        "stacked_predictions_probs_xgb_meta = meta_model_xgb.predict_proba(test_preds)[:, 1]\n",
        "fpr_stacked_xgb, tpr_stacked_xgb, _ = roc_curve(y_test, stacked_predictions_probs_xgb_meta)\n",
        "roc_auc_stacked_xgb = roc_auc_score(y_test, stacked_predictions_probs_xgb_meta)\n",
        "plt.plot(fpr_stacked_xgb, tpr_stacked_xgb, color='olive', linestyle='-', lw=2, label=f'Stacked Ensemble (XGB Meta) (AUC = {roc_auc_stacked_xgb:.3f})')\n",
        "\n",
        "\n",
        "# Plot the diagonal reference line\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
        "plt.title(\"ROC Curves: LSTM-AE vs. Ensemble Classifiers\", fontsize=14)\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{output_dir_comparison_plots}/ROC_Curve_Original_vs_Ensemble_Classifiers.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3L3SX45XvhZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate PR (AUC) of original vs All Classifiers except ensemble learning\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# --- Plotting PR Curve for Original Model vs. Individual Classifiers (Excluding Ensemble) ---\n",
        "\n",
        "# Define the output directory if not already defined\n",
        "output_dir_comparison_plots = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Comparison_Plots/\"\n",
        "os.makedirs(output_dir_comparison_plots, exist_ok=True)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Plot PR Curve for the Original Model (LSTM-AE Reconstruction Error)\n",
        "# Use reconstruction errors as the score directly. Higher error -> more likely anomaly.\n",
        "precision_original, recall_original, _ = precision_recall_curve(y_test, reconstruction_errors_test)\n",
        "pr_auc_original = average_precision_score(y_test, reconstruction_errors_test)\n",
        "plt.plot(recall_original, precision_original, color='darkorange', lw=2, label=f'LSTM-AE (Reconstruction Error) (AP = {pr_auc_original:.3f})')\n",
        "\n",
        "# Plot PR Curves for Individual Classifiers (trained on reconstruction errors)\n",
        "# Use predict_proba[:, 1] for standard classifiers. For OCSVM, use negative scores.\n",
        "\n",
        "# Logistic Regression\n",
        "precision_lr, recall_lr, _ = precision_recall_curve(y_test, best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1])\n",
        "pr_auc_lr = average_precision_score(y_test, best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1])\n",
        "plt.plot(recall_lr, precision_lr, color='blue', linestyle='--', lw=2, label=f'Logistic Regression (AP = {pr_auc_lr:.3f})')\n",
        "\n",
        "# MLP\n",
        "mlp_probs_test = mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten()\n",
        "precision_mlp, recall_mlp, _ = precision_recall_curve(y_test, mlp_probs_test)\n",
        "pr_auc_mlp = average_precision_score(y_test, mlp_probs_test)\n",
        "plt.plot(recall_mlp, precision_mlp, color='green', linestyle='--', lw=2, label=f'MLP (AP = {pr_auc_mlp:.3f})')\n",
        "\n",
        "# One-Class SVM\n",
        "ocsvm_scores_test = -ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1)).flatten()\n",
        "precision_ocsvm, recall_ocsvm, _ = precision_recall_curve(y_test, ocsvm_scores_test)\n",
        "pr_auc_ocsvm = average_precision_score(y_test, ocsvm_scores_test)\n",
        "plt.plot(recall_ocsvm, precision_ocsvm, color='red', linestyle='--', lw=2, label=f'One-Class SVM (AP = {pr_auc_ocsvm:.3f})')\n",
        "\n",
        "# K-Nearest Neighbors\n",
        "knn_probs_test = knn.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]\n",
        "precision_knn, recall_knn, _ = precision_recall_curve(y_test, knn_probs_test)\n",
        "pr_auc_knn = average_precision_score(y_test, knn_probs_test)\n",
        "plt.plot(recall_knn, precision_knn, color='purple', linestyle='--', lw=2, label=f'K-Nearest Neighbors (AP = {pr_auc_knn:.3f})')\n",
        "\n",
        "# XGBoost\n",
        "xgb_probs_test = xgb.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]\n",
        "precision_xgb, recall_xgb, _ = precision_recall_curve(y_test, xgb_probs_test)\n",
        "pr_auc_xgb = average_precision_score(y_test, xgb_probs_test)\n",
        "plt.plot(recall_xgb, precision_xgb, color='brown', linestyle='--', lw=2, label=f'XGBoost (AP = {pr_auc_xgb:.3f})')\n",
        "\n",
        "# Random Forest\n",
        "rf_probs_test = rf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1]\n",
        "precision_rf, recall_rf, _ = precision_recall_curve(y_test, rf_probs_test)\n",
        "pr_auc_rf = average_precision_score(y_test, rf_probs_test)\n",
        "plt.plot(recall_rf, precision_rf, color='pink', linestyle='--', lw=2, label=f'Random Forest (AP = {pr_auc_rf:.3f})')\n",
        "\n",
        "\n",
        "plt.xlabel(\"Recall\", fontsize=12)\n",
        "plt.ylabel(\"Precision\", fontsize=12)\n",
        "plt.title(\"Precision-Recall Curves: LSTM-AE vs. Individual Classifiers\", fontsize=14)\n",
        "plt.legend(loc=\"lower left\", fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{output_dir_comparison_plots}/PR_Curve_Original_vs_Individual_Classifiers.pdf\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-q73vIsSv7h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"statistical-analysis\"></a>\n",
        "## Statistical Test"
      ],
      "metadata": {
        "id": "O9IeecTK9zDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical Evaluation and Bootstrap Analysis"
      ],
      "metadata": {
        "id": "y1TSjTGH-D8q"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "from scipy.stats import ttest_rel, wilcoxon\n",
        "# from statsmodels.sandbox.stats.runs import mcnemar # Keeping this import, although statsmodels.stats.contingency_tables.mcnemar is used\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "from typing import Dict, Any, List\n",
        "import seaborn as sns # Ensure seaborn is imported for plotting\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, roc_curve, precision_recall_curve # Ensure all metrics are imported\n",
        "import os\n",
        "\n",
        "# Assuming y_test, reconstruction_errors_test, anomaly_predictions, best_clf, mlp_model, ocsvm, knn, xgb, rf are available from previous cells\n",
        "\n",
        "# Store results in dictionaries\n",
        "metric_distributions: Dict[str, Dict[str, List[float]]] = {}\n",
        "metric_summary: Dict[str, Dict[str, Dict[str, Any]]] = {}\n",
        "statistical_test_results: Dict[str, Dict[str, Dict[str, float]]] = {}\n",
        "\n",
        "# Define the models and their corresponding probabilities and binary predictions\n",
        "models = {\n",
        "    'LSTM-AE': {'probs': reconstruction_errors_test, 'preds': anomaly_predictions}, # Assuming reconstruction_errors_test is the \"score\" for LSTM-AE\n",
        "    'Logistic Regression': {'probs': best_clf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': best_clf.predict(reconstruction_errors_test.reshape(-1, 1))},\n",
        "    'MLP': {'probs': mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten(), 'preds': (mlp_model.predict(reconstruction_errors_test.reshape(-1, 1)).flatten() > 0.5).astype(int)},\n",
        "    'One-Class SVM': {'probs': -ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1)).flatten(), 'preds': (ocsvm.decision_function(reconstruction_errors_test.reshape(-1, 1)).flatten() < 0).astype(int)}, # Using negative scores for AUC/PR AUC, and thresholding at 0 for binary predictions\n",
        "    'KNN': {'probs': knn.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': knn.predict(reconstruction_errors_test.reshape(-1, 1))},\n",
        "    'XGBoost': {'probs': xgb.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': xgb.predict(reconstruction_errors_test.reshape(-1, 1))},\n",
        "    'Random Forest': {'probs': rf.predict_proba(reconstruction_errors_test.reshape(-1, 1))[:, 1], 'preds': rf.predict(reconstruction_errors_test.reshape(-1, 1))},\n",
        "    # Add ensemble models if desired\n",
        "    'Averaged Ensemble': {'probs': averaged_probs, 'preds': ensemble_predictions},\n",
        "    'Weighted Ensemble': {'probs': weighted_averaged_probs, 'preds': weighted_ensemble_predictions},\n",
        "    'Stacked (LR Meta)': {'probs': stacked_predictions_probs, 'preds': stacked_predictions},\n",
        "    'Stacked (XGB Meta)': {'probs': stacked_predictions_probs_xgb, 'preds': stacked_predictions_xgb},\n",
        "}\n",
        "\n",
        "n_iterations = 1000\n",
        "alpha = 0.05 # For confidence intervals and statistical significance\n",
        "\n",
        "print(\"Performing Bootstrap Testing...\")\n",
        "for model_name, data in models.items():\n",
        "    model_probs = data['probs']\n",
        "    model_preds = data['preds']\n",
        "\n",
        "    roc_aucs = []\n",
        "    pr_aucs = []\n",
        "    f1_scores = []\n",
        "\n",
        "    # Handle potential issues with OCSVM or other models having constant predictions/scores\n",
        "    # that would prevent AUC/F1 calculation on some bootstrapped samples.\n",
        "    # For simplicity, we'll skip iterations where metrics fail.\n",
        "    successful_iterations = 0\n",
        "    # Use a fixed random state for reproducibility of bootstrap samples themselves\n",
        "    bootstrap_rng = np.random.default_rng(seed=42)\n",
        "\n",
        "    # Store (y_true_sample, probs_sample) for plotting later\n",
        "    bootstrap_samples_for_plotting: List[tuple[np.ndarray, np.ndarray]] = []\n",
        "\n",
        "\n",
        "    while successful_iterations < n_iterations:\n",
        "        # Resample with replacement\n",
        "        indices = bootstrap_rng.choice(np.arange(len(y_test)), size=len(y_test), replace=True)\n",
        "\n",
        "        y_true_sample = y_test[indices]\n",
        "        probs_sample = model_probs[indices]\n",
        "        preds_sample = model_preds[indices]\n",
        "\n",
        "        try:\n",
        "            # Compute metrics for the sample\n",
        "            # Check for multiple classes in true labels and variation in scores/predictions\n",
        "            if len(np.unique(y_true_sample)) > 1:\n",
        "                 if len(np.unique(probs_sample)) > 1:\n",
        "                    roc_auc_sample = roc_auc_score(y_true_sample, probs_sample)\n",
        "                    pr_auc_sample = average_precision_score(y_true_sample, probs_sample)\n",
        "                    roc_aucs.append(roc_auc_sample)\n",
        "                    pr_aucs.append(pr_auc_sample)\n",
        "                 # We still try to compute F1 even if probs are constant, as preds might vary\n",
        "                 if len(np.unique(preds_sample)) > 1:\n",
        "                      f1_score_sample = f1_score(y_true_sample, preds_sample, zero_division=0)\n",
        "                      f1_scores.append(f1_score_sample)\n",
        "            # If only one class in y_true_sample, metrics like AUC, PR AUC, and standard F1 are not well-defined.\n",
        "            # We append np.nan or skip, skipping is better for distribution calculations later.\n",
        "\n",
        "            # Store sample for plotting if metrics were attempted (even if they failed)\n",
        "            # This ensures the lists for plotting match the iterations where we *tried* to compute metrics\n",
        "            bootstrap_samples_for_plotting.append((y_true_sample, probs_sample))\n",
        "\n",
        "            successful_iterations += 1 # Increment only if sample was generated and attempted\n",
        "\n",
        "        except Exception as e:\n",
        "            # print(f\"Skipping iteration {successful_iterations} for {model_name} due to error: {e}\")\n",
        "            # Still store the sample even if metric calculation failed, for consistent plotting sample count\n",
        "            bootstrap_samples_for_plotting.append((y_true_sample, probs_sample))\n",
        "            successful_iterations += 1 # Increment even on error to match total iterations\n",
        "\n",
        "\n",
        "    # Store distributions\n",
        "    metric_distributions[model_name] = {\n",
        "        'ROC AUC': roc_aucs,\n",
        "        'PR AUC': pr_aucs,\n",
        "        'F1 Score': f1_scores,\n",
        "        'bootstrap_samples_for_plotting': bootstrap_samples_for_plotting # Store samples for plotting\n",
        "    }\n",
        "\n",
        "    # Compute mean and 95% CI\n",
        "    metric_summary[model_name] = {}\n",
        "    for metric_name, values in {\n",
        "        'ROC AUC': roc_aucs,\n",
        "        'PR AUC': pr_aucs,\n",
        "        'F1 Score': f1_scores\n",
        "        }.items():\n",
        "        if values: # Ensure list is not empty\n",
        "            mean_val = np.mean(values)\n",
        "            # Compute confidence interval using percentile method\n",
        "            lower_bound = np.percentile(values, (alpha / 2) * 100)\n",
        "            upper_bound = np.percentile(values, 100 - (alpha / 2) * 100)\n",
        "            metric_summary[model_name][metric_name] = {\n",
        "                'mean': mean_val,\n",
        "                'ci_lower': lower_bound,\n",
        "                'ci_upper': upper_bound,\n",
        "                'distribution': values # Keep distribution for later plotting\n",
        "            }\n",
        "        else:\n",
        "             metric_summary[model_name][metric_name] = {\n",
        "                'mean': np.nan,\n",
        "                'ci_lower': np.nan,\n",
        "                'ci_upper': np.nan,\n",
        "                'distribution': []\n",
        "            }\n",
        "\n",
        "\n",
        "print(\"\\nPerforming Statistical Testing...\")\n",
        "# Collect bootstrap distributions for LSTM-AE explicitly after the loop finishes\n",
        "lstm_ae_roc_auc_dist = metric_distributions['LSTM-AE']['ROC AUC']\n",
        "lstm_ae_pr_auc_dist = metric_distributions['LSTM-AE']['PR AUC']\n",
        "lstm_ae_f1_dist = metric_distributions['LSTM-AE']['F1 Score']\n",
        "lstm_ae_preds = models['LSTM-AE']['preds'] # Binary predictions from LSTM-AE thresholding\n",
        "\n",
        "statistical_test_results['vs LSTM-AE'] = {}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qfLCnopsVdDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Testing Against LSTM-AE"
      ],
      "metadata": {
        "id": "LXddZBK8-X0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import ttest_rel, wilcoxon\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "\n",
        "for model_name, data in models.items():\n",
        "    if model_name == 'LSTM-AE':\n",
        "        continue  # Skip comparison with itself\n",
        "\n",
        "    model_roc_auc_dist = metric_distributions[model_name]['ROC AUC']\n",
        "    model_pr_auc_dist = metric_distributions[model_name]['PR AUC']\n",
        "    model_f1_dist = metric_distributions[model_name]['F1 Score']\n",
        "    model_preds = data['preds']\n",
        "\n",
        "    statistical_test_results['vs LSTM-AE'][model_name] = {}\n",
        "\n",
        "    # Ensure lengths match for paired tests\n",
        "    min_len_roc = min(len(lstm_ae_roc_auc_dist), len(model_roc_auc_dist))\n",
        "    if min_len_roc >= 2:\n",
        "        t_stat_roc, p_value_t_roc = ttest_rel(lstm_ae_roc_auc_dist[:min_len_roc], model_roc_auc_dist[:min_len_roc])\n",
        "        statistical_test_results['vs LSTM-AE'][model_name]['ROC AUC (T-test p)'] = p_value_t_roc\n",
        "\n",
        "        # Wilcoxon test fix: Ensure at least 2 distinct values\n",
        "        unique_values_lstm = len(set(lstm_ae_roc_auc_dist[:min_len_roc]))\n",
        "        unique_values_model = len(set(model_roc_auc_dist[:min_len_roc]))\n",
        "        if min_len_roc >= 2 and unique_values_lstm > 1 and unique_values_model > 1:\n",
        "            try:\n",
        "                w_stat_roc, p_value_w_roc = wilcoxon(lstm_ae_roc_auc_dist[:min_len_roc], model_roc_auc_dist[:min_len_roc])\n",
        "                statistical_test_results['vs LSTM-AE'][model_name]['ROC AUC (Wilcoxon p)'] = p_value_w_roc\n",
        "            except ValueError:\n",
        "                statistical_test_results['vs LSTM-AE'][model_name]['ROC AUC (Wilcoxon p)'] = np.nan\n",
        "        else:\n",
        "            statistical_test_results['vs LSTM-AE'][model_name]['ROC AUC (Wilcoxon p)'] = np.nan\n",
        "    else:\n",
        "        statistical_test_results['vs LSTM-AE'][model_name]['ROC AUC (T-test p)'] = np.nan\n",
        "        statistical_test_results['vs LSTM-AE'][model_name]['ROC AUC (Wilcoxon p)'] = np.nan\n",
        "\n",
        "    # Repeat fixes for PR AUC and F1 Score\n",
        "    for metric_name, lstm_dist, model_dist in zip(\n",
        "        ['PR AUC', 'F1 Score'],\n",
        "        [lstm_ae_pr_auc_dist, lstm_ae_f1_dist],\n",
        "        [model_pr_auc_dist, model_f1_dist]\n",
        "    ):\n",
        "        min_len = min(len(lstm_dist), len(model_dist))\n",
        "        if min_len >= 2:\n",
        "            t_stat, p_value_t = ttest_rel(lstm_dist[:min_len], model_dist[:min_len])\n",
        "            statistical_test_results['vs LSTM-AE'][model_name][f'{metric_name} (T-test p)'] = p_value_t\n",
        "\n",
        "            # Wilcoxon test fix\n",
        "            unique_values_lstm = len(set(lstm_dist[:min_len]))\n",
        "            unique_values_model = len(set(model_dist[:min_len]))\n",
        "            if min_len >= 2 and unique_values_lstm > 1 and unique_values_model > 1:\n",
        "                try:\n",
        "                    w_stat, p_value_w = wilcoxon(lstm_dist[:min_len], model_dist[:min_len])\n",
        "                    statistical_test_results['vs LSTM-AE'][model_name][f'{metric_name} (Wilcoxon p)'] = p_value_w\n",
        "                except ValueError:\n",
        "                    statistical_test_results['vs LSTM-AE']"
      ],
      "metadata": {
        "id": "FWqKcJslbSTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Comparison of Models"
      ],
      "metadata": {
        "id": "i9pB9A8U-dgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import ttest_rel, wilcoxon, mannwhitneyu\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "\n",
        "# Define the ensemble models\n",
        "ensemble_models = {\n",
        "    'Averaged Ensemble': {'probs': averaged_probs, 'preds': ensemble_predictions},\n",
        "    'Weighted Ensemble': {'probs': weighted_averaged_probs, 'preds': weighted_ensemble_predictions},\n",
        "    'Stacked (LR Meta)': {'probs': stacked_predictions_probs, 'preds': stacked_predictions},\n",
        "    'Stacked (XGB Meta)': {'probs': stacked_predictions_probs_xgb, 'preds': stacked_predictions_xgb},\n",
        "}\n",
        "\n",
        "# Store statistical test results for ensemble comparisons\n",
        "ensemble_statistical_test_results: Dict[str, Dict[str, Dict[str, float]]] = {}\n",
        "\n",
        "print(\"\\nPerforming Statistical Testing Among Ensemble Learners...\")\n",
        "\n",
        "ensemble_model_names = list(ensemble_models.keys())\n",
        "\n",
        "# Iterate through all unique pairs of ensemble models\n",
        "for i in range(len(ensemble_model_names)):\n",
        "    for j in range(i + 1, len(ensemble_model_names)):\n",
        "        model1_name = ensemble_model_names[i]\n",
        "        model2_name = ensemble_model_names[j]\n",
        "\n",
        "        model1_data = ensemble_models[model1_name]\n",
        "        model2_data = ensemble_models[model2_name]\n",
        "\n",
        "        model1_roc_auc_dist = np.array(metric_distributions[model1_name]['ROC AUC'], dtype=np.float64)\n",
        "        model1_pr_auc_dist = np.array(metric_distributions[model1_name]['PR AUC'], dtype=np.float64)\n",
        "        model1_f1_dist = np.array(metric_distributions[model1_name]['F1 Score'], dtype=np.float64)\n",
        "        model1_preds = model1_data['preds']\n",
        "\n",
        "        model2_roc_auc_dist = np.array(metric_distributions[model2_name]['ROC AUC'], dtype=np.float64)\n",
        "        model2_pr_auc_dist = np.array(metric_distributions[model2_name]['PR AUC'], dtype=np.float64)\n",
        "        model2_f1_dist = np.array(metric_distributions[model2_name]['F1 Score'], dtype=np.float64)\n",
        "        model2_preds = model2_data['preds']\n",
        "\n",
        "        comparison_key = f'{model1_name} vs {model2_name}'\n",
        "        ensemble_statistical_test_results[comparison_key] = {}\n",
        "\n",
        "        # Perform paired tests on metric distributions (ROC AUC, PR AUC, F1 Score)\n",
        "        for metric_name, dist1, dist2 in zip(\n",
        "            ['ROC AUC', 'PR AUC', 'F1 Score'],\n",
        "            [model1_roc_auc_dist, model1_pr_auc_dist, model1_f1_dist],\n",
        "            [model2_roc_auc_dist, model2_pr_auc_dist, model2_f1_dist]\n",
        "        ):\n",
        "            min_len = min(len(dist1), len(dist2))\n",
        "            if min_len >= 2:\n",
        "                # Paired t-test\n",
        "                t_stat, p_value_t = ttest_rel(dist1[:min_len], dist2[:min_len])\n",
        "                ensemble_statistical_test_results[comparison_key][f'{metric_name} (T-test p)'] = p_value_t\n",
        "\n",
        "                # Wilcoxon test\n",
        "                if len(set(dist1[:min_len])) > 1 and len(set(dist2[:min_len])) > 1:\n",
        "                    try:\n",
        "                        w_stat, p_value_w = wilcoxon(dist1[:min_len], dist2[:min_len])\n",
        "                        ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = p_value_w\n",
        "                    except ValueError:\n",
        "                        ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = np.nan\n",
        "                else:\n",
        "                    ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = np.nan\n",
        "\n",
        "                 # Mann-Whitney U test (alternative)\n",
        "                u_stat, p_value_u = mannwhitneyu(dist1[:min_len], dist2[:min_len])\n",
        "                ensemble_statistical_test_results[comparison_key][f'{metric_name} (Mann-Whitney p)'] = p_value_u\n",
        "\n",
        "\n",
        "            else:\n",
        "                ensemble_statistical_test_results[comparison_key][f'{metric_name} (T-test p)'] = np.nan\n",
        "                ensemble_statistical_test_results[comparison_key][f'{metric_name} (Wilcoxon p)'] = np.nan\n",
        "                ensemble_statistical_test_results[comparison_key][f'{metric_name} (Mann-Whitney p)'] = np.nan\n",
        "\n",
        "\n",
        "        # McNemar test on binary predictions\n",
        "        contingency_table_mcnemar = pd.crosstab(model1_preds, model2_preds).reindex(\n",
        "            pd.MultiIndex.from_product([[0, 1], [0, 1]], names=[f'{model1_name} Pred', f'{model2_name} Pred']), fill_value=0)\n",
        "\n",
        "        table_array_mcnemar = np.array([\n",
        "            [contingency_table_mcnemar.loc[(0, 0)], contingency_table_mcnemar.loc[(0, 1)]],\n",
        "            [contingency_table_mcnemar.loc[(1, 0)], contingency_table_mcnemar.loc[(1, 1)]]\n",
        "        ])\n",
        "\n",
        "        n01_scalar = int(table_array_mcnemar[0, 1])  # Model1 Pred 0, Model2 Pred 1\n",
        "        n10_scalar = int(table_array_mcnemar[1, 0])  # Model1 Pred 1, Model2 Pred 0\n",
        "\n",
        "        if n01_scalar + n10_scalar > 0:\n",
        "            mcnemar_result = mcnemar(table_array_mcnemar, exact=True)\n",
        "            ensemble_statistical_test_results[comparison_key]['McNemar p'] = mcnemar_result.pvalue\n",
        "        else:\n",
        "            ensemble_statistical_test_results[comparison_key]['McNemar p'] = 1.0\n",
        "\n",
        "\n",
        "# Now, format the results into a readable table\n",
        "print(\"\\nStatistical Test Results Among Ensemble Learners:\")\n",
        "\n",
        "ensemble_comparison_data = []\n",
        "for comparison, tests in ensemble_statistical_test_results.items():\n",
        "    row = {'Comparison': comparison}\n",
        "    for test_name, p_value in tests.items():\n",
        "         if not np.isnan(p_value):\n",
        "            row[test_name] = f\"{p_value:.4f}\"\n",
        "            row[f'Significant? ({test_name})'] = '*' if p_value < alpha else ''\n",
        "         else:\n",
        "            row[test_name] = \"N/A\"\n",
        "            row[f'Significant? ({test_name})'] = ''\n",
        "\n",
        "    ensemble_comparison_data.append(row)\n",
        "\n",
        "ensemble_comparison_df = pd.DataFrame(ensemble_comparison_data)\n",
        "\n",
        "# Define a desired column order for clarity\n",
        "ensemble_ordered_columns = ['Comparison']\n",
        "for metric_name in ['ROC AUC', 'PR AUC', 'F1 Score']:\n",
        "    ensemble_ordered_columns.append(f'{metric_name} (T-test p)')\n",
        "    ensemble_ordered_columns.append(f'Significant? ({metric_name} (T-test p))')\n",
        "    ensemble_ordered_columns.append(f'{metric_name} (Wilcoxon p)')\n",
        "    ensemble_ordered_columns.append(f'Significant? ({metric_name} (Wilcoxon p))')\n",
        "    ensemble_ordered_columns.append(f'{metric_name} (Mann-Whitney p)')\n",
        "    ensemble_ordered_columns.append(f'Significant? ({metric_name} (Mann-Whitney p))')\n",
        "\n",
        "ensemble_ordered_columns.append('McNemar p')\n",
        "ensemble_ordered_columns.append('Significant? (McNemar p)')\n",
        "\n",
        "# Filter columns to only include those present in the DataFrame\n",
        "final_ensemble_columns = [col for col in ensemble_ordered_columns if col in ensemble_comparison_df.columns]\n",
        "ensemble_comparison_df = ensemble_comparison_df[final_ensemble_columns]\n",
        "\n",
        "\n",
        "print(ensemble_comparison_df.to_string())\n",
        "\n",
        "# Optionally save this table\n",
        "output_dir_tables = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Tables/\"\n",
        "os.makedirs(output_dir_tables, exist_ok=True)\n",
        "ensemble_comparison_df.to_csv(f\"{output_dir_tables}/ensemble_statistical_comparison.csv\", index=False)\n",
        "\n",
        "# Prepare for LaTeX\n",
        "latex_ensemble_df = ensemble_comparison_df.copy()\n",
        "latex_ensemble_df.columns = latex_ensemble_df.columns.str.replace('_', '\\\\_')\n",
        "latex_ensemble_df.columns = latex_ensemble_df.columns.str.replace('%', '\\\\%')\n",
        "latex_ensemble_df.columns = latex_ensemble_df.columns.str.replace('(', '{(').str.replace(')', ')}')\n",
        "\n",
        "latex_ensemble_df.to_latex(f\"{output_dir_tables}/ensemble_statistical_comparison.tex\", index=False, float_format=\"%.4f\", escape=False)\n",
        "\n",
        "print(f\"\\nEnsemble statistical comparison table saved to {output_dir_tables}/ensemble_statistical_comparison.csv and .tex\")"
      ],
      "metadata": {
        "id": "37KIAfLMeKRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Tests Plots and Summary Table"
      ],
      "metadata": {
        "id": "P2X0O3Cn-ku9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Box/Violin plots of metrics\n",
        "metric_names = ['ROC AUC', 'PR AUC', 'F1 Score']\n",
        "for metric_name in metric_names:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plot_data = []\n",
        "    model_order = list(models.keys()) # Maintain a consistent order\n",
        "\n",
        "    for model_name in model_order:\n",
        "        if metric_name in metric_distributions[model_name] and metric_distributions[model_name][metric_name]:\n",
        "             plot_data.append(pd.DataFrame({\n",
        "                 'Model': model_name,\n",
        "                 metric_name: metric_distributions[model_name][metric_name]\n",
        "             }))\n",
        "\n",
        "\n",
        "    if plot_data:\n",
        "        combined_df = pd.concat(plot_data)\n",
        "        # Ensure the order is maintained in the plot\n",
        "        sns.boxplot(x='Model', y=metric_name, data=combined_df, palette='viridis', order=model_order)\n",
        "        # sns.violinplot(x='Model', y=metric_name, data=combined_df, palette='viridis', inner=\"quartile\", order=model_order) # Alternative violin plot\n",
        "        plt.title(f'{metric_name} Distribution from Bootstrap ({n_iterations} Iterations)')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        #plt.savefig(f\"{output_dir_bootstrap_plots}/{metric_name.replace(' ', '_')}_Bootstrap_BoxPlot.pdf\", dpi=600, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"No data available to plot {metric_name}.\")\n",
        "\n",
        "\n",
        "# Function to plot ROC/PR curve with bootstrap confidence bands\n",
        "def plot_curve_with_bands(model_name, bootstrap_samples_for_plotting, title_prefix, filename_prefix, curve_type='roc'):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    all_curves_interp = []\n",
        "    n_bootstrap_success = 0 # Count how many bootstrap samples were successfully used for curves\n",
        "\n",
        "    if curve_type == 'roc':\n",
        "        base_fpr = np.linspace(0, 1, 101) # Standard x-axis for interpolation\n",
        "        for y_true_sample, probs_sample in bootstrap_samples_for_plotting:\n",
        "\n",
        "            if len(np.unique(y_true_sample)) > 1 and len(np.unique(probs_sample)) > 1:\n",
        "                 fpr, tpr, _ = roc_curve(y_true_sample, probs_sample)\n",
        "                 interp_tpr = np.interp(base_fpr, fpr, tpr)\n",
        "                 interp_tpr[0] = 0.0 # Ensure start at (0,0)\n",
        "                 all_curves_interp.append(interp_tpr)\n",
        "                 n_bootstrap_success += 1\n",
        "\n",
        "        if n_bootstrap_success > 1: # Need at least 2 curves to compute band\n",
        "            mean_tpr = np.mean(all_curves_interp, axis=0)\n",
        "            # Using t-distribution for smaller sample sizes, but for n=1000, Z is fine\n",
        "            # std_tpr = np.std(all_curves_interp, axis=0)\n",
        "            # ci_width = 1.96 * std_tpr / np.sqrt(n_bootstrap_success)\n",
        "            # tprs_lower = mean_tpr - ci_width\n",
        "            # tprs_upper = mean_tpr + ci_width\n",
        "\n",
        "            # Percentile method for CI band - more robust to non-normal distributions\n",
        "            tprs_lower = np.percentile(all_curves_interp, (alpha / 2) * 100, axis=0)\n",
        "            tprs_upper = np.percentile(all_curves_interp, 100 - (alpha / 2) * 100, axis=0)\n",
        "\n",
        "\n",
        "            tprs_lower = np.maximum(tprs_lower, 0)\n",
        "            tprs_upper = np.minimum(tprs_upper, 1)\n",
        "\n",
        "            # Mean AUC from the bootstrap distribution (calculated earlier)\n",
        "            mean_auc_from_dist = np.mean(metric_distributions[model_name]['ROC AUC']) if metric_distributions[model_name]['ROC AUC'] else np.nan\n",
        "\n",
        "\n",
        "            plt.plot(base_fpr, mean_tpr, label=f'Mean ROC (AUC = {mean_auc_from_dist:.3f})')\n",
        "            plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label='95% CI')\n",
        "            plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
        "            plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
        "            plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
        "            plt.title(f'{title_prefix} ROC Curve with Bootstrap CI', fontsize=14)\n",
        "            plt.legend(loc=\"lower right\", fontsize=10)\n",
        "        else:\n",
        "             print(f\"Not enough successful bootstrap iterations ({n_bootstrap_success}) to plot ROC CI band for {title_prefix}.\")\n",
        "             # Plot the curve on the original test set if band cannot be plotted\n",
        "             # Check y_test directly for original plot\n",
        "             if len(np.unique(y_test)) > 1 and len(np.unique(models[model_name]['probs'])) > 1:\n",
        "                 fpr, tpr, _ = roc_curve(y_test, models[model_name]['probs'])\n",
        "                 auc_orig = roc_auc_score(y_test, models[model_name]['probs'])\n",
        "                 plt.plot(fpr, tpr, label=f'ROC (AUC = {auc_orig:.3f})')\n",
        "                 plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
        "                 plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
        "                 plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
        "                 plt.title(f'{title_prefix} ROC Curve (No CI band)', fontsize=14)\n",
        "                 plt.legend(loc=\"lower right\", fontsize=10)\n",
        "             else:\n",
        "                  print(f\"Skipping ROC plot for {title_prefix} - not enough unique values in original data.\")\n",
        "\n",
        "\n",
        "    elif curve_type == 'pr':\n",
        "         base_recall = np.linspace(0, 1, 101)\n",
        "         all_precisions_interp = []\n",
        "         n_bootstrap_success = 0\n",
        "\n",
        "         for y_true_sample, probs_sample in bootstrap_samples_for_plotting:\n",
        "\n",
        "            if len(np.unique(y_true_sample)) > 1 and len(np.unique(probs_sample)) > 1:\n",
        "                precision, recall, _ = precision_recall_curve(y_true_sample, probs_sample)\n",
        "                # Sort by recall for interpolation\n",
        "                sort_indices = np.argsort(recall)\n",
        "                recall = recall[sort_indices]\n",
        "                precision = precision[sort_indices]\n",
        "\n",
        "                # Handle interpolation carefully, especially at low recall\n",
        "                # Append (0, 1) point if not present\n",
        "                if recall[0] > 0:\n",
        "                    recall = np.insert(recall, 0, 0)\n",
        "                    precision = np.insert(precision, 0, 1)\n",
        "\n",
        "                # Append (proportion of positives, proportion of positives) at recall 1 if not present\n",
        "                # or ensure the last point is handled\n",
        "                if recall[-1] < 1.0:\n",
        "                     prop_pos = np.sum(y_true_sample) / len(y_true_sample) if len(y_true_sample) > 0 else 0\n",
        "                     recall = np.append(recall, 1.0)\n",
        "                     precision = np.append(precision, prop_pos) # Precision at recall 1 is proportion of positives\n",
        "\n",
        "                interp_precision = np.interp(base_recall, recall, precision)\n",
        "\n",
        "                all_precisions_interp.append(interp_precision)\n",
        "                n_bootstrap_success += 1\n",
        "\n",
        "         if n_bootstrap_success > 1: # Need at least 2 curves to compute band\n",
        "            mean_precision = np.mean(all_precisions_interp, axis=0)\n",
        "            # std_precision = np.std(all_precisions_interp, axis=0)\n",
        "            # ci_width_pr = 1.96 * std_precision / np.sqrt(n_bootstrap_success)\n",
        "            # precisions_lower = mean_precision - ci_width_pr\n",
        "            # precisions_upper = mean_precision + ci_width_pr\n",
        "\n",
        "            # Percentile method for CI band\n",
        "            precisions_lower = np.percentile(all_precisions_interp, (alpha / 2) * 100, axis=0)\n",
        "            precisions_upper = np.percentile(all_precisions_interp, 100 - (alpha / 2) * 100, axis=0)\n",
        "\n",
        "            precisions_lower = np.maximum(precisions_lower, 0)\n",
        "            precisions_upper = np.minimum(precisions_upper, 1)\n",
        "\n",
        "            # Mean AP from the bootstrap distribution (calculated earlier)\n",
        "            mean_ap_from_dist = np.mean(metric_distributions[model_name]['PR AUC']) if metric_distributions[model_name]['PR AUC'] else np.nan\n",
        "\n",
        "\n",
        "            plt.plot(base_recall, mean_precision, label=f'Mean PR (AP = {mean_ap_from_dist:.3f})')\n",
        "            plt.fill_between(base_recall, precisions_lower, precisions_upper, color='grey', alpha=.2, label='95% CI')\n",
        "            plt.xlabel(\"Recall\", fontsize=12)\n",
        "            plt.ylabel(\"Precision\", fontsize=12)\n",
        "            plt.title(f'{title_prefix} PR Curve with Bootstrap CI', fontsize=14)\n",
        "            plt.legend(loc=\"lower left\", fontsize=10)\n",
        "            plt.ylim([0.0, 1.05])\n",
        "            plt.xlim([0.0, 1.0])\n",
        "         else:\n",
        "              print(f\"Not enough successful bootstrap iterations ({n_bootstrap_success}) to plot PR CI band for {title_prefix}.\")\n",
        "              # Plot the curve on the original test set if band cannot be plotted\n",
        "              if len(np.unique(y_test)) > 1 and len(np.unique(models[model_name]['probs'])) > 1:\n",
        "                precision, recall, _ = precision_recall_curve(y_test, models[model_name]['probs'])\n",
        "                ap_orig = average_precision_score(y_test, models[model_name]['probs'])\n",
        "                plt.plot(recall, precision, label=f'PR (AP = {ap_orig:.3f})')\n",
        "                plt.xlabel(\"Recall\", fontsize=12)\n",
        "                plt.ylabel(\"Precision\", fontsize=12)\n",
        "                plt.title(f'{title_prefix} PR Curve (No CI band)', fontsize=14)\n",
        "                plt.legend(loc=\"lower left\", fontsize=10)\n",
        "                plt.ylim([0.0, 1.05])\n",
        "                plt.xlim([0.0, 1.0])\n",
        "              else:\n",
        "                   print(f\"Skipping PR plot for {title_prefix} - not enough unique values in original data.\")\n",
        "\n",
        "\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    #plt.savefig(f\"{output_dir_bootstrap_plots}/{filename_prefix}_{curve_type.upper()}_Curve_Bootstrap_CI.pdf\", dpi=600, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plot curves with bands for each model\n",
        "for model_name in models.keys():\n",
        "     plot_curve_with_bands(\n",
        "         model_name,\n",
        "         metric_distributions[model_name]['bootstrap_samples_for_plotting'],\n",
        "         model_name,\n",
        "         model_name.replace(' ', '_').replace('-', '_'),\n",
        "         curve_type='roc'\n",
        "         )\n",
        "     plot_curve_with_bands(\n",
        "         model_name,\n",
        "         metric_distributions[model_name]['bootstrap_samples_for_plotting'],\n",
        "         model_name,\n",
        "         model_name.replace(' ', '_').replace('-', '_'),\n",
        "         curve_type='pr'\n",
        "         )\n",
        "\n",
        "\n",
        "print(\"\\nGenerating Summary Table...\")\n",
        "\n",
        "summary_data = []\n",
        "for model_name in models.keys():\n",
        "    row: Dict[str, Any] = {'Model': model_name}\n",
        "\n",
        "    for metric_name in metric_names:\n",
        "        summary_info = metric_summary[model_name][metric_name]\n",
        "        mean = summary_info['mean']\n",
        "        ci_lower = summary_info['ci_lower']\n",
        "        ci_upper = summary_info['ci_upper']\n",
        "\n",
        "        if not np.isnan(mean):\n",
        "             row[f'{metric_name} Mean  95% CI'] = f\"{mean:.3f}  ({mean - ci_lower:.3f}, {ci_upper - mean:.3f})\"\n",
        "        else:\n",
        "             row[f'{metric_name} Mean  95% CI'] = \"N/A\"\n",
        "\n",
        "\n",
        "    if model_name != 'LSTM-AE':\n",
        "        vs_lstm_ae_tests = statistical_test_results['vs LSTM-AE'][model_name]\n",
        "        for test_name, p_value in vs_lstm_ae_tests.items():\n",
        "            if not np.isnan(p_value):\n",
        "                row[f'{test_name} (vs LSTM-AE)'] = f\"{p_value:.4f}\"\n",
        "                # Add significance flag (p < alpha)\n",
        "                row[f'Significant? ({test_name})'] = '*' if p_value < alpha else ''\n",
        "            else:\n",
        "                 row[f'{test_name} (vs LSTM-AE)'] = \"N/A\"\n",
        "                 row[f'Significant? ({test_name})'] = ''\n",
        "\n",
        "    summary_data.append(row)\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# Reorder columns for better readability\n",
        "ordered_columns = ['Model']\n",
        "for metric_name in metric_names:\n",
        "    ordered_columns.append(f'{metric_name} Mean  95% CI')\n",
        "    if metric_name != 'F1 Score': # Only ROC and PR have T-test and Wilcoxon on distribution\n",
        "        ordered_columns.append(f'{metric_name} (T-test p) (vs LSTM-AE)')\n",
        "        ordered_columns.append(f'Significant? ({metric_name} (T-test p))')\n",
        "        ordered_columns.append(f'{metric_name} (Wilcoxon p) (vs LSTM-AE)')\n",
        "        ordered_columns.append(f'Significant? ({metric_name} (Wilcoxon p))')\n",
        "    else: # F1 score has T-test and Wilcoxon\n",
        "        ordered_columns.append(f'{metric_name} (T-test p) (vs LSTM-AE)')\n",
        "        ordered_columns.append(f'Significant? ({metric_name} (T-test p))')\n",
        "        ordered_columns.append(f'{metric_name} (Wilcoxon p) (vs LSTM-AE)')\n",
        "        ordered_columns.append(f'Significant? ({metric_name} (Wilcoxon p))')\n",
        "\n",
        "\n",
        "ordered_columns.append('McNemar p (vs LSTM-AE)')\n",
        "ordered_columns.append('Significant? (McNemar p)')\n",
        "\n",
        "# Ensure only existing columns are included (handles LSTM-AE which has no vs tests)\n",
        "final_columns = [col for col in ordered_columns if col in summary_df.columns]\n",
        "summary_df = summary_df[final_columns]\n",
        "\n",
        "\n",
        "print(\"\\nSummary Table:\")\n",
        "print(summary_df.to_string()) # Use to_string to display the full table without truncation\n",
        "\n",
        "# Save the summary table to a file\n",
        "output_dir_tables = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Tables/\"\n",
        "os.makedirs(output_dir_tables, exist_ok=True)\n",
        "summary_df.to_csv(f\"{output_dir_tables}/model_performance_summary.csv\", index=False)\n",
        "summary_df.to_latex(f\"{output_dir_tables}/model_performance_summary.tex\", index=False, float_format=\"%.3f\")\n",
        "\n",
        "print(\"\\nAnalysis Complete.\")"
      ],
      "metadata": {
        "id": "7a9Ded6EWkgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Suggestions for Improvement or Extension\n",
        "# 1. Enhance Boxplot Aesthetics:\n",
        "# Consider adding swarm/strip plots for better visualization of distribution density:\n",
        "# sns.boxplot(...)  # Existing line\n",
        "# sns.stripplot(x='Model', y=metric_name, data=combined_df, color='black', alpha=0.3, jitter=True, size=3)\n",
        "# 2. Add Effect Sizes to Summary Table:\n",
        "# P-values tell you if a difference exists, not how large it is.\n",
        "# \tInclude Cohens d or Cliffs Delta for effect size on continuous metrics.\n",
        "# \tFor paired comparisons (e.g., ROC AUCs), Cohens d can be added easily:\n",
        "# def cohen_d(x, y):\n",
        "#     return (np.mean(x) - np.mean(y)) / np.sqrt((np.std(x, ddof=1) ** 2 + np.std(y, ddof=1) ** 2) / 2)\n",
        "# 3. Show Sample Size in Plots:\n",
        "# Add bootstrap sample count directly in the titles:\n",
        "# plt.title(f'{title_prefix} ROC Curve with Bootstrap CI\\n(N = {n_bootstrap_success} bootstrap samples)', fontsize=14)\n",
        "# 4. Make Alpha Adjustable:\n",
        "# Instead of a hardcoded alpha, you could parameterize it:\n",
        "# def plot_curve_with_bands(..., alpha=0.05):\n",
        "# 5. Save Plots in Multiple Formats (PDF, PNG):\n",
        "# To ensure compatibility across journals and presentations:\n",
        "# plt.savefig(f\"{output_dir}/{filename}.pdf\", dpi=600)\n",
        "# plt.savefig(f\"{output_dir}/{filename}.png\", dpi=300)\n",
        "# Optional: Parallelize Bootstrap Computation\n",
        "# If bootstrap iteration is time-consuming, you could parallelize curve computation using joblib:\n",
        "# from joblib import Parallel, delayed\n",
        "# def compute_bootstrap_curve(...):  # Wrapper function\n",
        "#     ...\n",
        "# results = Parallel(n_jobs=-1)(delayed(compute_bootstrap_curve)(...) for _ in range(n_iterations))\n",
        "# Optional: Include Metrics Distribution in Summary Table\n",
        "# Add SD or full distribution stats:\n",
        "# row[f'{metric_name} Std'] = f\"{np.std(metric_distributions[model_name][metric_name]):.3f}\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Enhance Boxplot Aesthetics: Add swarm/strip plots\n",
        "metric_names = ['ROC AUC', 'PR AUC', 'F1 Score']\n",
        "output_dir_bootstrap_plots = \"/content/drive/MyDrive/Final_Implementations/DynaBiome_Figures/Bootstrap_Plots/\"\n",
        "os.makedirs(output_dir_bootstrap_plots, exist_ok=True)\n",
        "\n",
        "for metric_name in metric_names:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plot_data = []\n",
        "    model_order = list(models.keys())  # Maintain a consistent order\n",
        "\n",
        "    for model_name in model_order:\n",
        "        if metric_name in metric_distributions[model_name] and metric_distributions[model_name][metric_name]:\n",
        "            plot_data.append(pd.DataFrame({\n",
        "                'Model': model_name,\n",
        "                metric_name: metric_distributions[model_name][metric_name]\n",
        "            }))\n",
        "\n",
        "    if plot_data:\n",
        "        combined_df = pd.concat(plot_data)\n",
        "        # Ensure the order is maintained in the plot\n",
        "        sns.boxplot(x='Model', y=metric_name, data=combined_df, palette='viridis', order=model_order)\n",
        "        # Add stripplot for individual data points\n",
        "        sns.stripplot(x='Model', y=metric_name, data=combined_df, color='black', alpha=0.3, jitter=True, size=3, order=model_order)\n",
        "\n",
        "        plt.title(f'{metric_name} Distribution from Bootstrap ({n_iterations} Iterations)')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{output_dir_bootstrap_plots}/{metric_name.replace(' ', '_')}_Bootstrap_BoxPlot_Stripplot.pdf\", dpi=600, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"No data available to plot {metric_name}.\")\n",
        "\n",
        "\n",
        "# Modify plot_curve_with_bands to show sample size in title\n",
        "def plot_curve_with_bands(model_name, bootstrap_samples_for_plotting, title_prefix, filename_prefix, curve_type='roc', n_bootstrap_total=None):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    all_curves_interp = []\n",
        "    n_bootstrap_success = 0  # Count how many bootstrap samples were successfully used for curves\n",
        "\n",
        "    if curve_type == 'roc':\n",
        "        base_fpr = np.linspace(0, 1, 101)  # Standard x-axis for interpolation\n",
        "        for y_true_sample, probs_sample in bootstrap_samples_for_plotting:\n",
        "            if len(np.unique(y_true_sample)) > 1 and len(np.unique(probs_sample)) > 1:\n",
        "                fpr, tpr, _ = roc_curve(y_true_sample, probs_sample)\n",
        "                interp_tpr = np.interp(base_fpr, fpr, tpr)\n",
        "                interp_tpr[0] = 0.0  # Ensure start at (0,0)\n",
        "                all_curves_interp.append(interp_tpr)\n",
        "                n_bootstrap_success += 1\n",
        "\n",
        "        if n_bootstrap_success > 1:  # Need at least 2 curves to compute band\n",
        "            mean_tpr = np.mean(all_curves_interp, axis=0)\n",
        "            tprs_lower = np.percentile(all_curves_interp, (alpha / 2) * 100, axis=0)\n",
        "            tprs_upper = np.percentile(all_curves_interp, 100 - (alpha / 2) * 100, axis=0)\n",
        "\n",
        "            tprs_lower = np.maximum(tprs_lower, 0)\n",
        "            tprs_upper = np.minimum(tprs_upper, 1)\n",
        "\n",
        "            mean_auc_from_dist = np.mean(metric_distributions[model_name]['ROC AUC']) if metric_distributions[model_name]['ROC AUC'] else np.nan\n",
        "\n",
        "            plt.plot(base_fpr, mean_tpr, label=f'Mean ROC (AUC = {mean_auc_from_dist:.3f})')\n",
        "            plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label='95% CI')\n",
        "            plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
        "            plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
        "            plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
        "            # Include sample size in title\n",
        "            plt.title(f'{title_prefix} ROC Curve with Bootstrap CI\\n(N = {n_bootstrap_total} bootstrap samples, {n_bootstrap_success} successful)', fontsize=14)\n",
        "            plt.legend(loc=\"lower right\", fontsize=10)\n",
        "        else:\n",
        "            print(f\"Not enough successful bootstrap iterations ({n_bootstrap_success}) to plot ROC CI band for {title_prefix}.\")\n",
        "            # Plot the curve on the original test set if band cannot be plotted\n",
        "            if len(np.unique(y_test)) > 1 and len(np.unique(models[model_name]['probs'])) > 1:\n",
        "                fpr, tpr, _ = roc_curve(y_test, models[model_name]['probs'])\n",
        "                auc_orig = roc_auc_score(y_test, models[model_name]['probs'])\n",
        "                plt.plot(fpr, tpr, label=f'ROC (AUC = {auc_orig:.3f})')\n",
        "                plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
        "                plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
        "                plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
        "                plt.title(f'{title_prefix} ROC Curve (No CI band)\\n(N = {len(y_test)} test samples)', fontsize=14) # Use test set size for this case\n",
        "                plt.legend(loc=\"lower right\", fontsize=10)\n",
        "            else:\n",
        "                print(f\"Skipping ROC plot for {title_prefix} - not enough unique values in original data.\")\n",
        "\n",
        "\n",
        "    elif curve_type == 'pr':\n",
        "        base_recall = np.linspace(0, 1, 101)\n",
        "        all_precisions_interp = []\n",
        "        n_bootstrap_success = 0\n",
        "\n",
        "        for y_true_sample, probs_sample in bootstrap_samples_for_plotting:\n",
        "            if len(np.unique(y_true_sample)) > 1 and len(np.unique(probs_sample)) > 1:\n",
        "                precision, recall, _ = precision_recall_curve(y_true_sample, probs_sample)\n",
        "                sort_indices = np.argsort(recall)\n",
        "                recall = recall[sort_indices]\n",
        "                precision = precision[sort_indices]\n",
        "\n",
        "                if recall[0] > 0:\n",
        "                    recall = np.insert(recall, 0, 0)\n",
        "                    precision = np.insert(precision, 0, 1)\n",
        "\n",
        "                if recall[-1] < 1.0:\n",
        "                    prop_pos = np.sum(y_true_sample) / len(y_true_sample) if len(y_true_sample) > 0 else 0\n",
        "                    recall = np.append(recall, 1.0)\n",
        "                    precision = np.append(precision, prop_pos)\n",
        "\n",
        "                interp_precision = np.interp(base_recall, recall, precision)\n",
        "\n",
        "                all_precisions_interp.append(interp_precision)\n",
        "                n_bootstrap_success += 1\n",
        "\n",
        "        if n_bootstrap_success > 1:  # Need at least 2 curves to compute band\n",
        "            mean_precision = np.mean(all_precisions_interp, axis=0)\n",
        "            precisions_lower = np.percentile(all_precisions_interp, (alpha / 2) * 100, axis=0)\n",
        "            precisions_upper = np.percentile(all_precisions_interp, 100 - (alpha / 2) * 100, axis=0)\n",
        "\n",
        "            precisions_lower = np.maximum(precisions_lower, 0)\n",
        "            precisions_upper = np.minimum(precisions_upper, 1)\n",
        "\n",
        "            mean_ap_from_dist = np.mean(metric_distributions[model_name]['PR AUC']) if metric_distributions[model_name]['PR AUC'] else np.nan\n",
        "\n",
        "            plt.plot(base_recall, mean_precision, label=f'Mean PR (AP = {mean_ap_from_dist:.3f})')\n",
        "            plt.fill_between(base_recall, precisions_lower, precisions_upper, color='grey', alpha=.2, label='95% CI')\n",
        "            plt.xlabel(\"Recall\", fontsize=12)\n",
        "            plt.ylabel(\"Precision\", fontsize=12)\n",
        "            # Include sample size in title\n",
        "            plt.title(f'{title_prefix} PR Curve with Bootstrap CI\\n(N = {n_bootstrap_total} bootstrap samples, {n_bootstrap_success} successful)', fontsize=14)\n",
        "            plt.legend(loc=\"lower left\", fontsize=10)\n",
        "            plt.ylim([0.0, 1.05])\n",
        "            plt.xlim([0.0, 1.0])\n",
        "        else:\n",
        "            print(f\"Not enough successful bootstrap iterations ({n_bootstrap_success}) to plot PR CI band for {title_prefix}.\")\n",
        "            # Plot the curve on the original test set if band cannot be plotted\n",
        "            if len(np.unique(y_test)) > 1 and len(np.unique(models[model_name]['probs'])) > 1:\n",
        "                precision, recall, _ = precision_recall_curve(y_test, models[model_name]['probs'])\n",
        "                ap_orig = average_precision_score(y_test, models[model_name]['probs'])\n",
        "                plt.plot(recall, precision, label=f'PR (AP = {ap_orig:.3f})')\n",
        "                plt.xlabel(\"Recall\", fontsize=12)\n",
        "                plt.ylabel(\"Precision\", fontsize=12)\n",
        "                plt.title(f'{title_prefix} PR Curve (No CI band)\\n(N = {len(y_test)} test samples)', fontsize=14) # Use test set size for this case\n",
        "                plt.legend(loc=\"lower left\", fontsize=10)\n",
        "                plt.ylim([0.0, 1.05])\n",
        "                plt.xlim([0.0, 1.0])\n",
        "            else:\n",
        "                print(f\"Skipping PR plot for {title_prefix} - not enough unique values in original data.\")\n",
        "\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_dir_bootstrap_plots}/{filename_prefix}_{curve_type.upper()}_Curve_Bootstrap_CI.pdf\", dpi=600, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Plot curves with bands using the modified function\n",
        "for model_name in models.keys():\n",
        "    plot_curve_with_bands(\n",
        "        model_name,\n",
        "        metric_distributions[model_name]['bootstrap_samples_for_plotting'],\n",
        "        model_name,\n",
        "        model_name.replace(' ', '_').replace('-', '_'),\n",
        "        curve_type='roc',\n",
        "        n_bootstrap_total=n_iterations # Pass the total number of iterations\n",
        "    )\n",
        "    plot_curve_with_bands(\n",
        "        model_name,\n",
        "        metric_distributions[model_name]['bootstrap_samples_for_plotting'],\n",
        "        model_name,\n",
        "        model_name.replace(' ', '_').replace('-', '_'),\n",
        "        curve_type='pr',\n",
        "        n_bootstrap_total=n_iterations # Pass the total number of iterations\n",
        "    )\n",
        "\n",
        "# Add Effect Sizes to Summary Table: Include Cohens d\n",
        "# Cohen's d for paired samples (appropriate for paired bootstrap samples)\n",
        "# We'll compare each model's bootstrap distribution to the LSTM-AE's distribution\n",
        "\n",
        "def cohen_d_paired(x, y):\n",
        "    \"\"\"Calculates Cohen's d for paired samples.\"\"\"\n",
        "    diff = np.array(x) - np.array(y)\n",
        "    mean_diff = np.mean(diff)\n",
        "    std_diff = np.std(diff, ddof=1) # Use ddof=1 for sample standard deviation\n",
        "    # Handle case where std_diff is zero\n",
        "    if std_diff == 0:\n",
        "        return np.inf if mean_diff != 0 else 0.0\n",
        "    return mean_diff / std_diff\n",
        "\n",
        "print(\"\\nCalculating Effect Sizes (Cohen's d vs LSTM-AE) and updating Summary Table...\")\n",
        "\n",
        "# Update summary_data list with effect sizes\n",
        "summary_data = []\n",
        "for model_name in models.keys():\n",
        "    row: Dict[str, Any] = {'Model': model_name}\n",
        "\n",
        "    for metric_name in metric_names:\n",
        "        summary_info = metric_summary[model_name][metric_name]\n",
        "        mean = summary_info['mean']\n",
        "        ci_lower = summary_info['ci_lower']\n",
        "        ci_upper = summary_info['ci_upper']\n",
        "        distribution = summary_info['distribution'] # Get the distribution\n",
        "\n",
        "        if not np.isnan(mean):\n",
        "            row[f'{metric_name} Mean (95% CI)'] = f\"{mean:.3f} ({ci_lower:.3f} - {ci_upper:.3f})\"\n",
        "        else:\n",
        "            row[f'{metric_name} Mean (95% CI)'] = \"N/A\"\n",
        "\n",
        "        # Calculate Cohen's d vs LSTM-AE if not the LSTM-AE model and distributions exist\n",
        "        if model_name != 'LSTM-AE' and distribution and metric_distributions['LSTM-AE'][metric_name]:\n",
        "             lstm_ae_dist = metric_distributions['LSTM-AE'][metric_name]\n",
        "             min_len = min(len(distribution), len(lstm_ae_dist))\n",
        "\n",
        "             if min_len >= 2: # Need at least 2 samples for Cohen's d\n",
        "                # Use the truncated distributions for Cohen's d to match the paired test length\n",
        "                d = cohen_d_paired(distribution[:min_len], lstm_ae_dist[:min_len])\n",
        "                row[f'{metric_name} Cohen\\'s d (vs LSTM-AE)'] = f\"{d:.3f}\"\n",
        "             else:\n",
        "                 row[f'{metric_name} Cohen\\'s d (vs LSTM-AE)'] = \"N/A\"\n",
        "        else:\n",
        "            row[f'{metric_name} Cohen\\'s d (vs LSTM-AE)'] = \"N/A\"\n",
        "\n",
        "\n",
        "    if model_name != 'LSTM-AE':\n",
        "        vs_lstm_ae_tests = statistical_test_results['vs LSTM-AE'][model_name]\n",
        "        for test_name, p_value in vs_lstm_ae_tests.items():\n",
        "            if not np.isnan(p_value):\n",
        "                row[f'{test_name} (vs LSTM-AE)'] = f\"{p_value:.4f}\"\n",
        "                row[f'Sig ({test_name})'] = '*' if p_value < alpha else ''\n",
        "            else:\n",
        "                row[f'{test_name} (vs LSTM-AE)'] = \"N/A\"\n",
        "                row[f'Sig ({test_name})'] = ''\n",
        "\n",
        "    summary_data.append(row)\n",
        "\n",
        "summary_df_updated = pd.DataFrame(summary_data)\n",
        "\n",
        "# Reorder columns for better readability with effect sizes\n",
        "ordered_columns_updated = ['Model']\n",
        "for metric_name in metric_names:\n",
        "    ordered_columns_updated.append(f'{metric_name} Mean (95% CI)')\n",
        "    ordered_columns_updated.append(f'{metric_name} Cohen\\'s d (vs LSTM-AE)')\n",
        "    # Add p-values and significance flags for statistical tests related to this metric\n",
        "    if metric_name != 'F1 Score':\n",
        "         ordered_columns_updated.append(f'{metric_name} (T-test p) (vs LSTM-AE)')\n",
        "         ordered_columns_updated.append(f'Sig ({metric_name} (T-test p))')\n",
        "         ordered_columns_updated.append(f'{metric_name} (Wilcoxon p) (vs LSTM-AE)')\n",
        "         ordered_columns_updated.append(f'Sig ({metric_name} (Wilcoxon p))')\n",
        "         ordered_columns_updated.append(f'{metric_name} (Mann-Whitney p) (vs LSTM-AE)')\n",
        "         ordered_columns_updated.append(f'Sig ({metric_name} (Mann-Whitney p))')\n",
        "\n",
        "    else:\n",
        "        ordered_columns_updated.append(f'{metric_name} (T-test p) (vs LSTM-AE)')\n",
        "        ordered_columns_updated.append(f'Sig ({metric_name} (T-test p))')\n",
        "        ordered_columns_updated.append(f'{metric_name} (Wilcoxon p) (vs LSTM-AE)')\n",
        "        ordered_columns_updated.append(f'Sig ({metric_name} (Wilcoxon p))')\n",
        "        ordered_columns_updated.append(f'{metric_name} (Mann-Whitney p) (vs LSTM-AE)')\n",
        "        ordered_columns_updated.append(f'Sig ({metric_name} (Mann-Whitney p))')\n",
        "\n",
        "\n",
        "# Add McNemar test results at the end\n",
        "ordered_columns_updated.append('McNemar p (vs LSTM-AE)')\n",
        "ordered_columns_updated.append('Sig (McNemar p)')\n",
        "\n",
        "\n",
        "# Ensure only existing columns are included\n",
        "final_columns_updated = [col for col in ordered_columns_updated if col in summary_df_updated.columns]\n",
        "summary_df_updated = summary_df_updated[final_columns_updated]\n",
        "\n",
        "\n",
        "print(\"\\nUpdated Summary Table with Effect Sizes and Detailed Statistical Tests:\")\n",
        "print(summary_df_updated.to_string())\n",
        "\n",
        "# Save the updated summary table\n",
        "summary_df_updated.to_csv(f\"{output_dir_tables}/model_performance_summary_with_effect_sizes.csv\", index=False)\n",
        "# Escape underscores for LaTeX\n",
        "summary_df_updated.columns = summary_df_updated.columns.str.replace('_', '\\\\_')\n",
        "summary_df_updated.to_latex(f\"{output_dir_tables}/model_performance_summary_with_effect_sizes.tex\", index=False, float_format=\"%.3f\", escape=False)"
      ],
      "metadata": {
        "id": "lvWoW2UkFib1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Generate the summary table from the collected data\n",
        "summary_data = []\n",
        "for model_name in models.keys():\n",
        "    row = {'Model': model_name}\n",
        "    # Add Mean and 95% CI for each metric\n",
        "    for metric_name in metric_names:\n",
        "        summary_info = metric_summary[model_name][metric_name]\n",
        "        mean = summary_info['mean']\n",
        "        ci_lower = summary_info['ci_lower']\n",
        "        ci_upper = summary_info['ci_upper']\n",
        "\n",
        "        if not np.isnan(mean):\n",
        "            # Format as \"Mean (Lower CI - Upper CI)\" for clarity\n",
        "            row[f'{metric_name} Mean (95% CI)'] = f\"{mean:.3f} ({ci_lower:.3f} - {ci_upper:.3f})\"\n",
        "        else:\n",
        "            row[f'{metric_name} Mean (95% CI)'] = \"N/A\"\n",
        "\n",
        "    # Add Cohen's d vs LSTM-AE\n",
        "    for metric_name in metric_names:\n",
        "        cohen_d_key = f'{metric_name} Cohen\\'s d (vs LSTM-AE)'\n",
        "        if cohen_d_key in summary_df_updated.columns: # Check if the column exists (will not exist for LSTM-AE row)\n",
        "            cohen_d_value = summary_df_updated.loc[summary_df_updated['Model'] == model_name, cohen_d_key].iloc[0]\n",
        "            row[cohen_d_key] = cohen_d_value\n",
        "        else:\n",
        "             row[cohen_d_key] = \"N/A\"\n",
        "\n",
        "\n",
        "    # Add Statistical Test p-values and Significance flags vs LSTM-AE\n",
        "    if model_name != 'LSTM-AE':\n",
        "        vs_lstm_ae_tests = statistical_test_results['vs LSTM-AE'][model_name]\n",
        "        for test_name, p_value in vs_lstm_ae_tests.items():\n",
        "            # Extract base metric name and test type\n",
        "            parts = test_name.rsplit(' ', 3) # Split from right 3 times\n",
        "            if len(parts) == 4: # Expected format \"Metric Name (Test Type p)\"\n",
        "                 metric_base_name = parts[0]\n",
        "                 test_type_part = f'{parts[1]} {parts[2]}' # e.g., \"(T-test p)\" or \"(Wilcoxon p)\"\n",
        "                 test_column_name = f'{metric_base_name} {test_type_part} (vs LSTM-AE)'\n",
        "                 sig_column_name = f'Sig ({metric_base_name} {test_type_part})'\n",
        "\n",
        "                 if test_column_name in summary_df_updated.columns:\n",
        "                    row[test_column_name] = summary_df_updated.loc[summary_df_updated['Model'] == model_name, test_column_name].iloc[0]\n",
        "                    row[sig_column_name] = summary_df_updated.loc[summary_df_updated['Model'] == model_name, sig_column_name].iloc[0]\n",
        "            elif test_name.startswith('McNemar'): # Special case for McNemar\n",
        "                test_column_name = 'McNemar p (vs LSTM-AE)'\n",
        "                sig_column_name = 'Sig (McNemar p)'\n",
        "                if test_column_name in summary_df_updated.columns:\n",
        "                    row[test_column_name] = summary_df_updated.loc[summary_df_updated['Model'] == model_name, test_column_name].iloc[0]\n",
        "                    row[sig_column_name] = summary_df_updated.loc[summary_df_updated['Model'] == model_name, sig_column_name].iloc[0]\n",
        "\n",
        "\n",
        "    summary_data.append(row)\n",
        "\n",
        "\n",
        "final_summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# Reorder columns for the final table structure\n",
        "# Ensure all potential columns from the previous calculations are included\n",
        "potential_columns = ['Model']\n",
        "for metric_name in metric_names:\n",
        "    potential_columns.append(f'{metric_name} Mean (95% CI)')\n",
        "    potential_columns.append(f'{metric_name} Cohen\\'s d (vs LSTM-AE)')\n",
        "    potential_columns.append(f'{metric_name} (T-test p) (vs LSTM-AE)')\n",
        "    potential_columns.append(f'Sig ({metric_name} (T-test p))')\n",
        "    potential_columns.append(f'{metric_name} (Wilcoxon p) (vs LSTM-AE)')\n",
        "    potential_columns.append(f'Sig ({metric_name} (Wilcoxon p))')\n",
        "    potential_columns.append(f'{metric_name} (Mann-Whitney p) (vs LSTM-AE)')\n",
        "    potential_columns.append(f'Sig ({metric_name} (Mann-Whitney p))') # Ensure Mann-Whitney is included\n",
        "\n",
        "potential_columns.append('McNemar p (vs LSTM-AE)')\n",
        "potential_columns.append('Sig (McNemar p)')\n",
        "\n",
        "\n",
        "# Filter columns to only include those present in the final_summary_df\n",
        "final_columns_ordered = [col for col in potential_columns if col in final_summary_df.columns]\n",
        "final_summary_df = final_summary_df[final_columns_ordered]\n",
        "\n",
        "\n",
        "print(\"\\nFinal Comprehensive Summary Table:\")\n",
        "# Display the table, preventing truncation\n",
        "print(final_summary_df.to_string())\n",
        "\n",
        "# Save the final comprehensive table\n",
        "final_summary_df.to_csv(f\"{output_dir_tables}/final_model_performance_summary.csv\", index=False)\n",
        "\n",
        "# Prepare for LaTeX: Escape special characters, especially underscores in column names\n",
        "latex_df = final_summary_df.copy()\n",
        "latex_df.columns = latex_df.columns.str.replace('_', '\\\\_')\n",
        "latex_df.columns = latex_df.columns.str.replace('%', '\\\\%') # Escape % as well\n",
        "latex_df.columns = latex_df.columns.str.replace('(', '{(').str.replace(')', ')}') # Handle parentheses in column names\n",
        "latex_df.to_latex(f\"{output_dir_tables}/final_model_performance_summary.tex\", index=False, float_format=\"%.3f\", escape=False)\n",
        "\n",
        "print(f\"\\nSummary table saved to {output_dir_tables}/final_model_performance_summary.csv and .tex\")\n"
      ],
      "metadata": {
        "id": "fuh2YmOjcT7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary Table"
      ],
      "metadata": {
        "id": "mupSgGLA-vm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
        "from typing import Dict, List, Any\n",
        "from scipy.stats import ttest_rel, wilcoxon, mannwhitneyu\n",
        "from statsmodels.stats.contingency_tables import mcnemar # Correct import for mcnemar\n",
        "\n",
        "# Assuming models, metric_summary, and statistical_test_results dictionaries\n",
        "# are populated from the previous code execution.\n",
        "# Also assuming metric_distributions is available and contains the bootstrap distributions.\n",
        "\n",
        "# Define the metrics we want in the table\n",
        "metric_names = ['ROC AUC', 'PR AUC', 'F1 Score']\n",
        "\n",
        "print(\"\\nGenerating Comprehensive Summary Table...\")\n",
        "\n",
        "summary_data = []\n",
        "for model_name in models.keys():\n",
        "    row: Dict[str, Any] = {'Model': model_name}\n",
        "\n",
        "    # Add Mean and 95% CI for each metric\n",
        "    for metric_name in metric_names:\n",
        "        # Ensure the metric exists for the model in metric_summary\n",
        "        if model_name in metric_summary and metric_name in metric_summary[model_name]:\n",
        "            summary_info = metric_summary[model_name][metric_name]\n",
        "            mean = summary_info['mean']\n",
        "            ci_lower = summary_info['ci_lower']\n",
        "            ci_upper = summary_info['ci_upper']\n",
        "\n",
        "            if not np.isnan(mean):\n",
        "                # Format as \"Mean (Lower CI - Upper CI)\"\n",
        "                row[f'{metric_name} Mean (95% CI)'] = f\"{mean:.3f} ({ci_lower:.3f} - {ci_upper:.3f})\"\n",
        "            else:\n",
        "                row[f'{metric_name} Mean (95% CI)'] = \"N/A\"\n",
        "        else:\n",
        "            row[f'{metric_name} Mean (95% CI)'] = \"N/A\" # Metric info missing\n",
        "\n",
        "\n",
        "    # Add Statistical Test p-values and Significance flags vs LSTM-AE\n",
        "    if model_name != 'LSTM-AE':\n",
        "        # Ensure test results exist for the model\n",
        "        if 'vs LSTM-AE' in statistical_test_results and model_name in statistical_test_results['vs LSTM-AE']:\n",
        "            vs_lstm_ae_tests = statistical_test_results['vs LSTM-AE'][model_name]\n",
        "\n",
        "            # Define the specific tests to include and their order\n",
        "            tests_to_include = {\n",
        "                'ROC AUC (T-test p)': 'ROC AUC (T-test p) (vs LSTM-AE)',\n",
        "                'ROC AUC (Wilcoxon p)': 'ROC AUC (Wilcoxon p) (vs LSTM-AE)',\n",
        "                # 'ROC AUC (Mann-Whitney p)': 'ROC AUC (Mann-Whitney p) (vs LSTM-AE)', # Include if Mann-Whitney was used\n",
        "                'PR AUC (T-test p)': 'PR AUC (T-test p) (vs LSTM-AE)',\n",
        "                'PR AUC (Wilcoxon p)': 'PR AUC (Wilcoxon p) (vs LSTM-AE)',\n",
        "                 # 'PR AUC (Mann-Whitney p)': 'PR AUC (Mann-Whitney p) (vs LSTM-AE)', # Include if Mann-Whitney was used\n",
        "                'F1 Score (T-test p)': 'F1 Score (T-test p) (vs LSTM-AE)',\n",
        "                'F1 Score (Wilcoxon p)': 'F1 Score (Wilcoxon p) (vs LSTM-AE)',\n",
        "                # 'F1 Score (Mann-Whitney p)': 'F1 Score (Mann-Whitney p) (vs LSTM-AE)', # Include if Mann-Whitney was used\n",
        "                'McNemar p': 'McNemar p (vs LSTM-AE)'\n",
        "            }\n",
        "\n",
        "            for test_key, column_name in tests_to_include.items():\n",
        "                p_value = vs_lstm_ae_tests.get(test_key) # Use .get to handle potentially missing keys\n",
        "\n",
        "                if p_value is not None and not np.isnan(p_value):\n",
        "                    row[column_name] = f\"{p_value:.4f}\"\n",
        "                    # Add significance flag (p < 0.05)\n",
        "                    row[f'Significant? ({test_key})'] = '*' if p_value < 0.05 else ''\n",
        "                else:\n",
        "                    row[column_name] = \"N/A\"\n",
        "                    row[f'Significant? ({test_key})'] = ''\n",
        "        else:\n",
        "            # Add N/A for all test columns if no test results are found for the model\n",
        "            # This ensures consistent columns even if testing failed for a model\n",
        "            tests_to_include = {\n",
        "                'ROC AUC (T-test p)': 'ROC AUC (T-test p) (vs LSTM-AE)',\n",
        "                'ROC AUC (Wilcoxon p)': 'ROC AUC (Wilcoxon p) (vs LSTM-AE)',\n",
        "                # 'ROC AUC (Mann-Whitney p)': 'ROC AUC (Mann-Whitney p) (vs LSTM-AE)',\n",
        "                'PR AUC (T-test p)': 'PR AUC (T-test p) (vs LSTM-AE)',\n",
        "                'PR AUC (Wilcoxon p)': 'PR AUC (Wilcoxon p) (vs LSTM-AE)',\n",
        "                # 'PR AUC (Mann-Whitney p)': 'PR AUC (Mann-Whitney p) (vs LSTM-AE)',\n",
        "                'F1 Score (T-test p)': 'F1 Score (T-test p) (vs LSTM-AE)',\n",
        "                'F1 Score (Wilcoxon p)': 'F1 Score (Wilcoxon p) (vs LSTM-AE)',\n",
        "                # 'F1 Score (Mann-Whitney p)': 'F1 Score (Mann-Whitney p) (vs LSTM-AE)',\n",
        "                'McNemar p': 'McNemar p (vs LSTM-AE)'\n",
        "            }\n",
        "            for test_key, column_name in tests_to_include.items():\n",
        "                row[column_name] = \"N/A\"\n",
        "                row[f'Significant? ({test_key})'] = ''\n",
        "\n",
        "\n",
        "    summary_data.append(row)\n",
        "\n",
        "# Create the DataFrame from the collected data\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# Define the desired column order\n",
        "# This ensures the table has a consistent structure\n",
        "ordered_columns = ['Model']\n",
        "for metric_name in metric_names:\n",
        "    ordered_columns.append(f'{metric_name} Mean (95% CI)')\n",
        "    # Add test columns related to this metric\n",
        "    if metric_name == 'ROC AUC':\n",
        "        ordered_columns.append('ROC AUC (T-test p) (vs LSTM-AE)')\n",
        "        ordered_columns.append('Significant? (ROC AUC (T-test p))')\n",
        "        ordered_columns.append('ROC AUC (Wilcoxon p) (vs LSTM-AE)')\n",
        "        ordered_columns.append('Significant? (ROC AUC (Wilcoxon p))')\n",
        "        # ordered_columns.append('ROC AUC (Mann-Whitney p) (vs LSTM-AE)') # Include if used\n",
        "        # ordered_columns.append('Significant? (ROC AUC (Mann-Whitney p))')\n",
        "    elif metric_name == 'PR AUC':\n",
        "        ordered_columns.append('PR AUC (T-test p) (vs LSTM-AE)')\n",
        "        ordered_columns.append('Significant? (PR AUC (T-test p))')\n",
        "        ordered_columns.append('PR AUC (Wilcoxon p) (vs LSTM-AE)')\n",
        "        ordered_columns.append('Significant? (PR AUC (Wilcoxon p))')\n",
        "        # ordered_columns.append('PR AUC (Mann-Whitney p) (vs LSTM-AE)') # Include if used\n",
        "        # ordered_columns.append('Significant? (PR AUC (Mann-Whitney p))')\n",
        "    elif metric_name == 'F1 Score':\n",
        "        ordered_columns.append('F1 Score (T-test p) (vs LSTM-AE)')\n",
        "        ordered_columns.append('Significant? (F1 Score (T-test p))')\n",
        "        ordered_columns.append('F1 Score (Wilcoxon p) (vs LSTM-AE)')\n",
        "        ordered_columns.append('Significant? (F1 Score (Wilcoxon p))')\n",
        "        # ordered_columns.append('F1 Score (Mann-Whitney p) (vs LSTM-AE)') # Include if used\n",
        "        # ordered_columns.append('Significant? (F1 Score (Mann-Whitney p))')\n",
        "\n",
        "\n",
        "# Add McNemar's test results\n",
        "ordered_columns.append('McNemar p (vs LSTM-AE)')\n",
        "ordered_columns.append('Significant? (McNemar p)')\n",
        "\n",
        "\n",
        "# Filter columns to only include those that actually exist in the DataFrame\n",
        "final_columns = [col for col in ordered_columns if col in summary_df.columns]\n",
        "summary_df = summary_df[final_columns]\n",
        "\n",
        "\n",
        "print(\"\\nSummary Table:\")\n",
        "# Use to_string() to print the entire DataFrame without truncation\n",
        "print(summary_df.to_string())\n",
        "\n",
        "# You might want to save this DataFrame to a file (e.g., CSV, Excel)\n",
        "# Example: Save to CSV\n",
        "# summary_df.to_csv(\"model_performance_summary.csv\", index=False)"
      ],
      "metadata": {
        "id": "UFz071cwnPkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d05b6f12"
      },
      "source": [
        "#pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29f556fa"
      },
      "source": [
        "<a name=\"usage\"></a>\n",
        "## Usage\n",
        "\n",
        "To use this project, follow these steps:\n",
        "\n",
        "1.  **Setup**: Ensure you have the necessary dependencies installed as described in the [Setup](#setup) section.\n",
        "2.  **Data**: Place your `asv_interpretability_dataset_modified.csv` file in the `/content/` directory or update the data loading code to point to your data location.\n",
        "3.  **Run the Notebook**: Execute the code cells in the notebook sequentially. The notebook is designed to perform the data preprocessing, model training, evaluation, and interpretation steps.\n",
        "4.  **Analyze Results**: Review the generated plots and the summary table to understand the model performance and feature importance. The output files (PDFs and CSVs) will be saved in the specified directories.\n",
        "5.  **Adapt the Code**: Modify the code as needed for your specific dataset or research questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44e4dcc1"
      },
      "source": [
        "<a name=\"license\"></a>\n",
        "## License"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b6d9819"
      },
      "source": [
        "                                 Apache License\n",
        "                           Version 2.0, January 2004\n",
        "                        http://www.apache.org/licenses/\n",
        "\n",
        "   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
        "\n",
        "   1. Definitions.\n",
        "\n",
        "      \"License\" shall mean the terms and conditions for use, reproduction,\n",
        "      and distribution as defined by Sections 1 through 9 of this document.\n",
        "\n",
        "      \"Licensor\" shall mean the copyright owner or entity authorized by\n",
        "      the copyright owner that is granting the License.\n",
        "\n",
        "      \"Legal Entity\" shall mean the union of the acting entity and all\n",
        "      other entities that control, are controlled by, or are under common\n",
        "      control with that entity. For the purposes of this definition,\n",
        "      \"control\" means (i) the power, direct or indirect, to cause the\n",
        "      direction or management of such entity, whether by contract or\n",
        "      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n",
        "      outstanding shares, or (iii) beneficial ownership of such entity.\n",
        "\n",
        "      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n",
        "      exercising permissions granted by this License.\n",
        "\n",
        "      \"Source\" form shall mean the preferred form for making modifications,\n",
        "      including but not limited to software source code, documentation\n",
        "      source, and configuration files.\n",
        "\n",
        "      \"Object\" form shall mean any form resulting from mechanical\n",
        "      transformation or translation of a Source form, including but\n",
        "      not limited to compiled object code, generated documentation,\n",
        "      and conversions to other media types.\n",
        "\n",
        "      \"Contribution\" shall mean any work of authorship, including\n",
        "      the original version of the Work and any modifications or additions\n",
        "      to that Work or Derivative Works thereof, that is intentionally\n",
        "      submitted to Licensor for inclusion in the Work by the copyright owner\n",
        "      or by an individual or Legal Entity authorized to submit on behalf of\n",
        "      the copyright owner. For the purposes of this definition, \"submitted\"\n",
        "      means any form of electronic, verbal, or written communication sent\n",
        "      to the Licensor or its representatives, including but not limited to\n",
        "      communication on electronic mailing lists, source code control systems,\n",
        "      and issue tracking systems that are managed by, or on behalf of, the\n",
        "      Licensor for the purpose of discussing and improving the Work, but\n",
        "      excluding communication that is conspicuously marked or otherwise\n",
        "      designated in writing by the copyright owner as \"Not a Contribution.\"\n",
        "\n",
        "      \"Contributor\" shall mean any individual or Legal Entity on behalf of\n",
        "      whom a Contribution has been received by Licensor and subsequently\n",
        "      accepted by Licensor.\n",
        "\n",
        "      \"Derivative Works\" shall mean any work, whether in Source or Object\n",
        "      form, that is based on (or derived from) the Work and for which the\n",
        "      editorial revisions, annotations, elaborations, or other modifications\n",
        "      represent, as a whole, an original work of authorship. For this\n",
        "      definition, Derivative Works shall not include works that remain\n",
        "      separable from, or merely link (or bind by name) to the interfaces of\n",
        "      the Work and Derivative Works thereof.\n",
        "\n",
        "      \"Distribution\" shall mean any means by which You provide, make available,\n",
        "      or otherwise transfer copies of the Work.\n",
        "\n",
        "      \"Submit\" means any act of handing in, sending, or transmitting\n",
        "      a Contribution to the Licensor or its representatives.\n",
        "\n",
        "      \"Contributor Grant\" shall mean the Patent Grant and Copyright Grant\n",
        "      granted by Contributors to Licensor.\n",
        "\n",
        "      \"Third Party\" shall mean any individual or entity other than, or in\n",
        "      addition to, Licensor and You.\n",
        "\n",
        "      \"Work\" shall mean the work of authorship, whether in Source or Object\n",
        "      form, made available under the License, as indicated by a copyright\n",
        "      notice that is included in or attached to the work (an example is\n",
        "      provided in the Appendix below).\n",
        "\n",
        "   2. Grant of Copyright License. Subject to the terms and conditions of\n",
        "      this License, each Contributor hereby grants to You a perpetual,\n",
        "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
        "      copyright license to reproduce, prepare Derivative Works of,\n",
        "      publicly display, publicly perform, sublicense, and distribute the\n",
        "      Work and such Derivative Works in Source or Object form.\n",
        "\n",
        "   3. Grant of Patent License. Subject to the terms and conditions of\n",
        "      this License, each Contributor hereby grants to You a perpetual,\n",
        "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
        "      (except as stated in this section) patent license to make, have made,\n",
        "      use, offer to sell, sell, import, and otherwise transfer the Work,\n",
        "      where such license applies only to those patent claims licensed by\n",
        "      each Contributor that are necessarily infringed by their Contribution(s)\n",
        "      alone or by combination of their Contribution(s) with the Work to which\n",
        "      such Contribution(s) was submitted. If You institute patent litigation\n",
        "      against any entity (including a cross-claim or counterclaim in a lawsuit)\n",
        "      alleging that the Work or a Contribution incorporated within the Work\n",
        "      constitutes direct or contributory patent infringement, then any patent\n",
        "      licenses granted to You under this License for that Work shall terminate\n",
        "      as of the date such litigation is filed.\n",
        "\n",
        "   4. Redistribution. You may reproduce and distribute copies of the Work or\n",
        "      Derivative Works thereof in any medium, with or without modifications,\n",
        "      and in Source or Object form, provided that You meet the following\n",
        "      conditions:\n",
        "\n",
        "      (a) You must give any other recipients of the Work or Derivative Works\n",
        "          a copy of this License; and\n",
        "\n",
        "      (b) You must cause any modified files to carry prominent notices stating\n",
        "          that You changed the files; and\n",
        "\n",
        "      (c) You must retain, in the Source form of any Derivative Works that\n",
        "          You distribute, all copyright, patent, trademark, and attribution\n",
        "          notices from the Source form of the Work, excluding those notices that\n",
        "          do not pertain to any part of the Derivative Works; and\n",
        "\n",
        "      (d) If the Work includes a \"NOTICE\" text file as part of its\n",
        "          Distribution, then any Derivative Works that You distribute must\n",
        "          include a readable copy of the attribution notices contained\n",
        "          within such NOTICE file, excluding those notices that do not\n",
        "          pertain to any part of the Derivative Works, in at least one\n",
        "          of the following places: within a NOTICE text file distributed\n",
        "          as part of the Derivative Works; within the Source form or\n",
        "          documentation, if provided along with the Derivative Works; or,\n",
        "          within a display generated by the Derivative Works, if and\n",
        "          wherever such third-party notices normally appear. The contents\n",
        "          of the NOTICE file are for informational purposes only and do\n",
        "          not modify the License. You may add Your own attribution notices\n",
        "          within Derivative Works that You distribute, alongside, or as\n",
        "          an addendum to the NOTICE text from the Work, provided that\n",
        "          such additional attribution notices cannot be construed as\n",
        "          modifying the License.\n",
        "\n",
        "      You may add Your own copyright statement to Your modifications and may\n",
        "      provide additional or different license terms and conditions for use,\n",
        "      reproduction, or distribution of Your modifications, or for any\n",
        "      such Derivative Works as a whole, provided Your use, reproduction,\n",
        "      and distribution of the Work otherwise complies with the conditions stated in this License.\n",
        "\n",
        "   5. Submission of Contributions. Unless You explicitly state otherwise,\n",
        "      any Contribution intentionally submitted for inclusion in the Work by\n",
        "      You to the Licensor shall be under the terms and conditions of this\n",
        "      License, without any additional terms or conditions.\n",
        "      Notwithstanding the above, nothing herein shall supersede or modify\n",
        "      the terms of any separate license agreement you may have executed\n",
        "      with Licensor regarding such Contributions.\n",
        "\n",
        "   6. Trademarks. This License does not grant permission to use the trade\n",
        "      names, trademarks, service marks, or product names of the Licensor,\n",
        "      except as required for reasonable and customary usage in describing the\n",
        "      origin of the Work and reproducing the content of the NOTICE file.\n",
        "\n",
        "   7. Disclaimer of Warranty. Unless required by applicable law or\n",
        "      agreed to in writing, Licensor provides the Work (and each Contributor\n",
        "      provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR\n",
        "      CONDITIONS OF ANY KIND, either express or implied, including, without\n",
        "      limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT,\n",
        "      MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely\n",
        "      responsible for determining the appropriateness of using or\n",
        "      redistributing the Work and assume any risks associated with Your\n",
        "      exercise of permissions under this License.\n",
        "\n",
        "   8. Limitation of Liability. In no event and under no legal theory,\n",
        "      whether in tort (including negligence), contract, or otherwise,\n",
        "      unless required by applicable law (such as deliberate and grossly\n",
        "      negligent acts) or agreed to in writing, shall any Contributor be\n",
        "      liable to You for damages, including any direct, indirect, special,\n",
        "      incidental, or consequential damages of any character arising as a\n",
        "      result of this License or out of the use or inability to use the\n",
        "      Work (including but not limited to damages for loss of goodwill,\n",
        "      work stoppage, computer failure or malfunction, or any and all\n",
        "      other commercial damages or losses), even if such Contributor\n",
        "      has been advised of the possibility of such damages.\n",
        "\n",
        "   9. Accepting Warranty or Additional Liability. While redistributing\n",
        "      the Work or Derivative Works thereof, You may choose to offer,\n",
        "      and charge a fee for, acceptance of support, warranty, indemnity,\n",
        "      or other liability obligations and/or rights consistent with this\n",
        "      License. However, in accepting such obligations, You may act only\n",
        "      on Your own behalf and on Your sole responsibility, not on behalf\n",
        "      of any other Contributor, and only if You agree to indemnify,\n",
        "      defend, and hold each Contributor harmless for any liability\n",
        "      incurred by, or claims asserted against, such Contributor by reason\n",
        "      of your accepting any such warranty or additional liability.\n",
        "\n",
        "   END OF TERMS AND CONDITIONS\n",
        "\n",
        "   APPENDIX: How to apply the Apache License to your work.\n",
        "\n",
        "      To apply the Apache License to your document, just attach the following\n",
        "      boilerplate notice, with the fields enclosed by brackets \"{}\" replaced\n",
        "      with your own identifying information. (Don't include the brackets!)\n",
        "      The text should be enclosed in the appropriate comment syntax for the\n",
        "      file format. We also recommend that a file named NOTICE is distributed\n",
        "      along with the Work; whatever the default distribution named methods\n",
        "      of the Source Code work, such as a text file along with binary distributions.\n",
        "\n",
        "\n",
        "   Copyright [yyyy] [name of author organization]\n",
        "\n",
        "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "   you may not use this file except in compliance with the License.\n",
        "   You may obtain a copy of the License at:\n",
        "\n",
        "       http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "   Unless required by applicable law or agreed to in writing, software\n",
        "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "   See the License for the specific language governing permissions and\n",
        "   limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96d22f99"
      },
      "source": [
        "<a name=\"contact\"></a>\n",
        "## Contact\n",
        "\n",
        "If you have any questions or suggestions, feel free to contact:\n",
        "\n",
        "Mr. Awais Qureshi (MS-IT),\n",
        "NUST SEECS,\n",
        "aqureshi.dphd19seecs@seecs.edu.pk"
      ]
    }
  ]
}